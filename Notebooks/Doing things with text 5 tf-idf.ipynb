{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 5\n",
    "\n",
    "## TF-IDF on multiple texts _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `sklearn.TfidfVectorizer`: Transforms text data into TF-IDF features for text analysis and machine learning.\n",
    "- `sklearn.CountVectorizer`: Converts text data into a matrix of token counts for analysis.\n",
    "- `sklearn.cosine_similarity`: Computes the cosine similarity between vectors for similarity analysis.\n",
    "- `sklearn.linear_kernel`: Calculates the linear kernel (dot product) for vector similarity or SVMs.\n",
    "- `pandas`: Provides data structures and tools for data manipulation and analysis.\n",
    "- `matplotlib.pyplot`: Creates static, animated, and interactive visualizations in Python.\n",
    "- `seaborn`: Builds on matplotlib for enhanced statistical data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/outdir/')\n",
    "outdir.mkdir(parents=True, exist_ok=True) # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.txt\"))\n",
    "\n",
    "dataset = 'dataset' # give a name to your dataset for outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the data\n",
    "\n",
    "Stores al text files from indir as strings in a list input_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "all_docs = []\n",
    "\n",
    "for infile in allfiles:\n",
    "    # add filename to list file_names\n",
    "    file_names.append(infile.stem) \n",
    "    # open the file and do something with it, close when done\n",
    "    with open(infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "    # add text to list of text strings all_docs       \n",
    "    all_docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tf-idf analysis\n",
    "\n",
    "From: https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "See also: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "len(file_names) == len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_tfidf_csv = outdir / f'{save_corpus(dataset)}_tfidf_csv/'\n",
    "outdir_tfidf_csv.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outdir_tfidf_png = outdir / f'{save_corpus(dataset)}_tfidf_png/'\n",
    "outdir_tfidf_png.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "# Set number of n top terms\n",
    "n = 20\n",
    "\n",
    "# Loop through each document\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # Get the original filename without the extension\n",
    "    original_filename = file_names[counter]\n",
    "\n",
    "    # Construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, \n",
    "                                              columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Output to CSV\n",
    "    one_doc_as_df.to_csv(outdir_tfidf_csv / f'{original_filename}_tfidf.csv')\n",
    "\n",
    "    # Select top terms\n",
    "    top_terms = one_doc_as_df[:n]\n",
    "    \n",
    "    # Create a separate figure for each document\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(top_terms['term'], top_terms['score'])\n",
    "    plt.xlabel('Top terms')\n",
    "    plt.ylabel('tf-idf score')\n",
    "    plt.title(f'Top {n} terms with highest tf-idf scores in {original_filename}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.savefig(outdir_tfidf_png / f\"{original_filename}_tfidf.png\", dpi=300) # Save individual chart with the original filename\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "count_matrix = vectorizer2.fit_transform(all_docs)\n",
    "cosine_sim = cosine_similarity(count_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn cosine_sim into pandas dataframe to visualize in heatmap. Name columns and index after correct year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_docs = pd.DataFrame(cosine_sim, columns = file_names)\n",
    "df_all_docs.index = df_all_docs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "sns.heatmap(df_all_docs.corr(), square=True, cmap='RdYlGn', ax=ax)\n",
    "plt.title('Heatmap of Cosine Similarity scores')\n",
    "plt.savefig(outdir / f'{dataset}_heatmap.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
