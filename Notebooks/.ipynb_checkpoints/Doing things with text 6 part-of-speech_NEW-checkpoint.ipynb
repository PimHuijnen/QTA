{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 6\n",
    "\n",
    "Part-of-speech, Named entity recognition _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download spacy model (only the first time)\n",
    "\n",
    "See https://spacy.io/models for the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.cli.download import download\n",
    "# download(model=\"nl_core_news_sm\") # en_core_web_sm is the standard model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets.\n",
    "- `collections.defaultdict`: A dictionary-like object that provides default values for missing keys.\n",
    "- `collections.Counter`: A dictionary subclass for counting hashable objects.\n",
    "- `spacy`: A natural language processing library for tasks like tokenization, tagging, and entity recognition.\n",
    "- `nltk.bigrams`: Creates bigrams (2-word combinations) from a sequence.\n",
    "- `nltk.collocations`: Provides tools for identifying collocations (frequent word pairings).\n",
    "- `nltk.FreqDist`: Calculates frequency distributions of items in a dataset.\n",
    "- `nltk.collocations.*`: Includes utilities for finding collocations like bigram or trigram associations.\n",
    "- `nltk.WordPunctTokenizer`: Tokenizes text into words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk import collocations\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the correct (language-specific) spacy model, load the default spacy stop word list and add words as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "nlp.max_length = 2000000\n",
    "nlp.Defaults.stop_words |= {'the'} # add words as 'word', 'word', 'word'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/outdir')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.txt\"))\n",
    "\n",
    "dataset = 'dataset' # give a name to your dataset for outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the data and create a dataframe \n",
    "Df with the texts in \"text\" column and the file name (=date) in \"file_name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "for infile in allfiles:\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "        results[\"year\"].append(infile.stem)\n",
    "        results[\"text\"].append(text)\n",
    "        \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn \"file_name\" column into datetime and set as index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4a: Set 'year' column to index and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"date\"] = pd.to_datetime(df[\"year\"], format =\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"year\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate and print Part Of Speech (POS) tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5a: Create POS tags for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process all texts with SpaCy\n",
    "docs = [(year, nlp(text)) for year, text in zip(df.index, df['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5b: Initiate function and set n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_to_word(pos_tag):\n",
    "    if pos_tag == 'ADJ':\n",
    "        return 'adjective'\n",
    "    elif pos_tag == 'NOUN':\n",
    "        return 'noun'\n",
    "    elif pos_tag == 'PROPN':\n",
    "        return 'proper noun'\n",
    "    elif pos_tag == 'VERB':\n",
    "        return 'verb'\n",
    "    elif pos_tag == 'SYM':\n",
    "        return 'symbol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract tokens by POS tag\n",
    "def extract_top_tokens(docs, pos_tag, n):\n",
    "    for year, doc in docs:\n",
    "        tokens = [token.text for token in doc\n",
    "                  if not token.is_stop and not token.is_punct and token.pos_ == pos_tag]\n",
    "        \n",
    "        print(f'These are the top {n} {pos_tag_to_word(pos_tag)}s from {year}:')\n",
    "        \n",
    "        token_freq = Counter(tokens)\n",
    "        common_tokens = token_freq.most_common(n)\n",
    "        print(common_tokens)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what the the spacy abbreviations stand for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"SYM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5c: Print top adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_top_tokens(docs, 'ADJ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5d: Print top nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_top_tokens(docs, 'NOUN', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5e: Print top proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_top_tokens(docs, 'PROPN', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5f: Print top verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_top_tokens(docs, 'VERB', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Print ALL words with a particular POS-tag (NOUN, ADJ, VERB, PRON, PROPN, SYM, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING: Only use this code if your dataset is manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print all words with a specific POS tag\n",
    "def print_all_words_by_pos(docs, pos_tag):\n",
    "    for year, doc in docs:\n",
    "        # Extract tokens with the desired POS tag\n",
    "        tokens = [token.text for token in doc if token.pos_ == pos_tag]\n",
    "        \n",
    "        # Print all tokens for the year\n",
    "        print(f'All {pos_tag.lower()}s in {year}:')\n",
    "        print(tokens)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_words_by_pos(docs, 'NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create Named Entity list \n",
    "(doesn't work well for Dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lst = nlp.pipe_labels['ner']\n",
    "print(len(ner_lst))\n",
    "print(ner_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_list = ['EVENT', 'FAC', 'LAW', 'LOC', 'MONEY', 'ORG', 'PERSON', 'PRODUCT']\n",
    "\n",
    "for year, text in zip(df.index, df['text']):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print('Named entities in ' + year +':')\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in NER_list:\n",
    "            print(ent.text,  ent.label_)\n",
    "            print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: POS Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['']\n",
    "windows = [10] # add or change to smaller/larger window\n",
    "algorithms = ['likelihood', 'pmi'] # 'likelihood', 'pmi', 'raw_freq'\n",
    "coll_to_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter tokens by POS tags\n",
    "def filter_tokens_by_pos(doc, pos_tags):\n",
    "    return [token.text for token in doc if token.pos_ in pos_tags and not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_coll = outdir / f'{save_corpus(dataset)}_pos-collocations'\n",
    "outdir_coll.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "for term in search_terms:\n",
    "    for window in windows:\n",
    "        for algorithm in algorithms:\n",
    "            outfile_coll = f'{term}_{algorithm}_pos-collocations_{window}.txt'\n",
    "            outpath_coll = outdir_coll / outfile_coll\n",
    "            \n",
    "            with open(outpath_coll, 'a') as f:\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset))\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset), file=f)\n",
    "                for year, doc in docs:\n",
    "                    # Filter tokens by desired POS types (e.g., nouns and proper nouns)\n",
    "                    filtered_tokens = filter_tokens_by_pos(doc, {'ADJ'})\n",
    "                    \n",
    "                    # Tokenize the filtered tokens\n",
    "                    tokens = WordPunctTokenizer().tokenize(' '.join(filtered_tokens))\n",
    "\n",
    "                    bigram_measures = collocations.BigramAssocMeasures()\n",
    "                    word_fd = FreqDist(tokens)\n",
    "                    bigram_fd = FreqDist(bigrams(tokens))\n",
    "                    finder = BigramCollocationFinder(word_fd, bigram_fd, window_size=window)\n",
    "\n",
    "                    #preprocessing: remove short words and stop words (see above) if only relevant for collocations\n",
    "                    #finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in stopwords)\n",
    "        \n",
    "                    if algorithm == 'likelihood': \n",
    "                        scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "                    elif algorithm == 'pmi': \n",
    "                        scored = finder.score_ngrams(bigram_measures.pmi) \n",
    "                    else: \n",
    "                        scored = finder.score_ngrams(bigram_measures.raw_freq) \n",
    "                  \n",
    "                    # Group bigrams by first word in bigram                                       \n",
    "                    prefix_keys = defaultdict(list)\n",
    "                    for key, scores in scored:\n",
    "                        prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "                    # Sort keyed bigrams by strongest association                                  \n",
    "                    for key in prefix_keys:\n",
    "                        prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "                    # Print top collocations of term.\n",
    "                    print(str(year) + ':')\n",
    "                    print(str(year) + ':', file=f)\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n')\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n', file=f)\n",
    "                    print('\\n')\n",
    "                    print('\\n', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
