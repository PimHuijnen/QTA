{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Things with Text 1\n",
    "\n",
    "## Cleaning a text document\n",
    "\n",
    "This notebook introduces basic text processing to help you analyze a single text document. In this notebook, you'll learn to clean up (preprocess) a text to prepare it for analysis. Don't worry if you're new to Python â€” we'll guide you through each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a library for working with text. To use it, you'll need to download some additional language data the first time you use NLTK. Run the following cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required packages\n",
    "import nltk\n",
    "nltk.download('punkt_tab')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Packages\n",
    "\n",
    "Here, we're loading a few packages to help with text cleaning:\n",
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `BeautifulSoup`: To clean up HTML text.\n",
    "- `unicodedata`: For handling special characters.\n",
    "- `re`: For regular expressions (patterns used for finding and cleaning text).\n",
    "- `nltk.tokenize`: For splitting text into individual words.\n",
    "- `nltk.corpus.stopwords`: A collection of common words like 'the', 'and', 'is', which are often removed in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Input and Output Paths\n",
    "\n",
    "Define where your text file is located (input) and where you want to save your processed text (output). You will use `os.path.join()` to define your paths. This approach is cross-platform, meaning it will work on Windows, macOS, and Linux.\n",
    "\n",
    "Replace 'path', 'to', 'your', 'input', 'folder' with the actual paths to your files. It is not necessary for the output folder to exist. If it doesn't, this code will create it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = Path('/path/to/your/input/folder/')\n",
    "outdir = Path('/path/to/your/output/folder/')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load Your Text Document\n",
    "\n",
    "Now, let's load your text file. Make sure the file is in the folder you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'infile.txt' # change 'infile' for actual file name\n",
    "file_path = indir / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "print(text[:400])  # Show a sample of the loaded text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Cleaning\n",
    "\n",
    "Now, we'll go through several steps to clean the text. Each step has a brief explanation and an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5a: Lowercase the Text\n",
    "Converting text to lowercase helps standardize it, so 'Python' and 'python' are treated as the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5b: Remove HTML Tags\n",
    "If your text includes HTML, let's remove it to keep only the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "text = soup.get_text()\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5c: Remove Non-Alphabetic Characters\n",
    "This step removes characters that are not letters, leaving only the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-alphabetic characters\n",
    "text = re.sub('[^a-z\\s\\']', '', text)\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5d: Tokenize the Text\n",
    "Tokenizing splits the text into individual words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:20])  # Show the first few tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5e: Remove Stopwords\n",
    "Stopwords are common words like 'the', 'is', and 'and'. Removing them can make the text more focused on meaningful words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_tokens = [word for word in tokens if word not in stop_words]\n",
    "print(clean_tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5f: Remove short words\n",
    "\n",
    "Short words might be less meaningful. Set n to a number that you think is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove short words\n",
    "n = 4\n",
    "clean_tokens = [word for word in clean_tokens if len(word) >= n]\n",
    "print(clean_tokens[:20])  # Preview first 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save Your Cleaned Text\n",
    "\n",
    "Finally, let's save the cleaned text tokens back to a new file (with the same name as the original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned tokens to a new file\n",
    "output_file = outdir / f'{file_path.stem}.txt'\n",
    "with open(output_file, 'w', encoding='utf8') as f:\n",
    "    f.write(' '.join(clean_tokens))\n",
    "print('Cleaned text saved to:', output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Counting Words Before and After Preprocessing\n",
    "\n",
    "To understand the impact of cleaning, we'll compare the total word count before and after preprocessing. This gives insight into how many words (such as punctuation, stopwords) were removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of words before cleaning\n",
    "word_count_before = len(tokens)\n",
    "print('Total number of words before cleaning:', word_count_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of words after cleaning\n",
    "word_count_after = len(clean_tokens)\n",
    "print('Total number of words after cleaning:', word_count_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the difference in word count\n",
    "print('Words removed during cleaning:', word_count_before - word_count_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
