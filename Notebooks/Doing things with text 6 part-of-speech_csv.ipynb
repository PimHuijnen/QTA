{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 6\n",
    "\n",
    "Part-of-speech, Named entity recognition _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Download spacy model (only the first time)\n",
    "\n",
    "See https://spacy.io/models for the available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.cli.download import download\n",
    "# download(model=\"nl_core_news_sm\") # en_core_web_sm is the standard model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets.\n",
    "- `collections.defaultdict`: A dictionary-like object that provides default values for missing keys.\n",
    "- `collections.Counter`: A dictionary subclass for counting hashable objects.\n",
    "- `spacy`: A natural language processing library for tasks like tokenization, tagging, and entity recognition.\n",
    "- `nltk.bigrams`: Creates bigrams (2-word combinations) from a sequence.\n",
    "- `nltk.collocations`: Provides tools for identifying collocations (frequent word pairings).\n",
    "- `nltk.FreqDist`: Calculates frequency distributions of items in a dataset.\n",
    "- `nltk.collocations.*`: Includes utilities for finding collocations like bigram or trigram associations.\n",
    "- `nltk.WordPunctTokenizer`: Tokenizes text into words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk import collocations\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the correct (language-specific) spacy model, load the default spacy stop word list and add words as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "nlp.max_length = 2000000\n",
    "nlp.Defaults.stop_words |= {'the'} # add words as 'word', 'word', 'word'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = Path('/Users/huijn001/desktop/test2/')\n",
    "outdir = Path('/Users/huijn001/desktop/output/')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.csv\"))\n",
    "\n",
    "dataset = 'dataset' # give a name to your dataset for outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/huijn001/desktop/test2/dataset_clean.csv\n"
     ]
    }
   ],
   "source": [
    "for file in allfiles:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load the data and create a dataframe \n",
    "Df with the texts in \"text_clean_str\" column and the file name (=date) in \"file_name\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4a: Create dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "for file in allfiles:\n",
    "    try:\n",
    "        # Load the CSV into a DataFrame\n",
    "        df_infile = pd.read_csv(file, sep='\\t')\n",
    "        \n",
    "        # Ensure 'date' and 'text' are available\n",
    "        if 'date' in df_infile.columns:\n",
    "            date_column = df_infile['date']\n",
    "        else:\n",
    "            date_column = df_infile.index  # Use index if 'date' column doesn't exist\n",
    "\n",
    "        # Append the data to results\n",
    "        for date, text in zip(date_column, df_infile['text_clean_str']):\n",
    "            results[\"date\"].append(date)\n",
    "            results[\"text\"].append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                               text\n",
      "0  12-11-1980  mijnheer voorzitter begin verontrustende pakke...\n",
      "1    4-6-1980  mijnheer voorzitter toch eens zeggen bijna tri...\n",
      "2    2-6-1980  mevrou voorzitter sluit graag staatssecretaris...\n",
      "3   13-2-1980  emancipatiebeleid heeft ruime aandacht kamer g...\n",
      "4   13-2-1980  door leden bischoff heemskerck spek vlist jans...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4b: Set 'date' column to datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"date\")\n",
    "df.index = pd.to_datetime(df.index, format =\"%d-%m-%Y\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         text\n",
      "date                                                         \n",
      "1970-11-18  uitgangspunt mijnheer voorzitter daarin bestaa...\n",
      "1970-11-18  mijnheer voorzitter toelichting amendement vra...\n",
      "1970-11-18  mijnheer voorzitter toelichting amendement vra...\n",
      "1970-11-18  uitgangspunt mijnheer voorzitter daarin bestaa...\n",
      "1973-10-09  mijnheer voorzitter begin enkele woorden wijde...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4c: Group by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'year' and aggregate 'text' by concatenation and 'num_words' by summation\n",
    "df = df.groupby(df.index.year).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text\n",
      "date                                                   \n",
      "1970  uitgangspunt mijnheer voorzitter daarin bestaa...\n",
      "1973  mijnheer voorzitter begin enkele woorden wijde...\n",
      "1974  mijnheer voorzitter geachte afgevaardigde heer...\n",
      "1975  mijnheer voorzitter over volgende onderwerpen ...\n",
      "1976  mijnheer voorzitter denkt over onderwijs emanc...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate and print Part Of Speech (POS) tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5a: Create POS tags for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process all texts with SpaCy\n",
    "docs = [(year, nlp(text)) for year, text in zip(df.index, df['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5b: Initiate function and set n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_to_word(pos_tag):\n",
    "    if pos_tag == 'ADJ':\n",
    "        return 'adjective'\n",
    "    elif pos_tag == 'NOUN':\n",
    "        return 'noun'\n",
    "    elif pos_tag == 'PROPN':\n",
    "        return 'proper noun'\n",
    "    elif pos_tag == 'VERB':\n",
    "        return 'verb'\n",
    "    elif pos_tag == 'SYM':\n",
    "        return 'symbol'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract tokens by POS tag\n",
    "def extract_top_tokens(docs, pos_tag, n):\n",
    "    for year, doc in docs:\n",
    "        tokens = [token.text for token in doc\n",
    "                  if not token.is_stop and not token.is_punct and token.pos_ == pos_tag]\n",
    "        \n",
    "        print(f'These are the top {n} {pos_tag_to_word(pos_tag)}s from {year}:')\n",
    "        \n",
    "        token_freq = Counter(tokens)\n",
    "        common_tokens = token_freq.most_common(n)\n",
    "        print(common_tokens)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what the the spacy abbreviations stand for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'symbol'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"SYM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5c: Print top adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 adjectives from 1970:\n",
      "[('geachte', 20), ('bijzonder', 12), ('maatschappelijk', 8), ('grote', 6), ('duidelijk', 6), ('nationale', 6), ('goede', 6), ('verschillende', 6), ('circulaire', 4), ('nederlandse', 4)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1973:\n",
      "[('tweede', 6), ('politieke', 4), ('hoger', 4), ('gelijke', 4), ('diep', 2), ('tragisch', 2), ('grote', 2), ('militair', 2), ('eenzijdige', 2), ('bestand', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1974:\n",
      "[('geachte', 16), ('tweede', 14), ('bijzonder', 8), ('verschillende', 8), ('maatschappelijke', 8), ('langedijk', 6), ('grote', 6), ('schriftelijk', 6), ('gedeeltelijk', 6), ('nederlandse', 6)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1975:\n",
      "[('sociale', 22), ('verschillende', 20), ('bijzonder', 16), ('nader', 14), ('jongeren', 12), ('vrouwe', 12), ('korte', 10), ('natuurlijk', 10), ('bezig', 10), ('regionaal', 8)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1976:\n",
      "[('maatschappelijk', 24), ('nodig', 14), ('open', 12), ('nieuwe', 12), ('werkloze', 12), ('duidelijk', 10), ('heel', 10), ('algemeen', 8), ('maatschappelijke', 8), ('goede', 8)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1977:\n",
      "[('duidelijk', 10), ('goed', 6), ('volledig', 6), ('nuttig', 4), ('belangrijkste', 4), ('maatschappelijk', 4), ('vrouwe', 4), ('kenbaar', 4), ('gelijke', 4), ('sociale', 4)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1978:\n",
      "[('eigenlijk', 10), ('goede', 10), ('vorige', 8), ('open', 8), ('snel', 6), ('bezig', 6), ('jongeren', 6), ('algemeen', 6), ('volledige', 6), ('sociale', 6)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1979:\n",
      "[('sociale', 52), ('maatschappelijk', 46), ('algemene', 38), ('goede', 30), ('maatschappelijke', 28), ('financiële', 28), ('duidelijk', 26), ('nodig', 24), ('algemeen', 24), ('tweede', 22)]\n",
      "\n",
      "\n",
      "These are the top 10 adjectives from 1980:\n",
      "[('sociale', 44), ('nieuwe', 38), ('geachte', 38), ('goed', 32), ('extra', 30), ('grote', 30), ('tweede', 30), ('jongeren', 28), ('goede', 28), ('duidelijk', 26)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_top_tokens(docs, 'ADJ', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5d: Print top nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 nouns from 1970:\n",
      "[('voorzitter', 22), ('zaak', 20), ('mijnheer', 16), ('premie', 16), ('pensioenregeling', 16), ('verschillen', 12), ('financiering', 12), ('mening', 12), ('stand', 12), ('kinderdagverblijven', 12)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1973:\n",
      "[('mensen', 14), ('onderwijs', 14), ('voorzitter', 6), ('basis', 6), ('benadering', 6), ('kabinet', 6), ('mevrouw', 6), ('verhaal', 6), ('mijnheer', 4), ('woorden', 4)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1974:\n",
      "[('mijnheer', 20), ('voorzitter', 20), ('onderwijs', 20), ('heer', 16), ('minister', 14), ('onderzoek', 14), ('punt', 12), ('aantal', 12), ('opmerkingen', 12), ('beleid', 10)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1975:\n",
      "[('minister', 36), ('zaken', 30), ('werk', 30), ('nota', 26), ('beleid', 22), ('positie', 18), ('mensen', 18), ('situatie', 16), ('jaar', 16), ('mogelijkheden', 16)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1976:\n",
      "[('beleid', 32), ('werk', 30), ('onderwijs', 26), ('mensen', 22), ('voorzitter', 20), ('samenleving', 18), ('plaats', 16), ('mijnheer', 14), ('prioriteiten', 14), ('cultuur', 14)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1977:\n",
      "[('motie', 26), ('beleid', 10), ('aanbevelingen', 10), ('kabinet', 8), ('vrouwen', 8), ('mannen', 8), ('heer', 8), ('werk', 8), ('toelichting', 8), ('aandacht', 8)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1978:\n",
      "[('vrouwen', 26), ('mensen', 24), ('werk', 20), ('onderwijs', 20), ('minister', 16), ('plaats', 14), ('werkloosheid', 14), ('kinderopvang', 14), ('arbeid', 14), ('groep', 12)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1979:\n",
      "[('vrouwen', 94), ('vrouw', 86), ('staatssecretaris', 84), ('beleid', 72), ('werk', 68), ('alimentatie', 62), ('mevrouw', 50), ('huwelijk', 50), ('vraag', 46), ('kinderopvang', 44)]\n",
      "\n",
      "\n",
      "These are the top 10 nouns from 1980:\n",
      "[('staatssecretaris', 86), ('beleid', 78), ('vrouwen', 70), ('jaar', 64), ('emancipatie', 62), ('werk', 56), ('zaken', 54), ('nota', 54), ('overleg', 52), ('mevrouw', 52)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_top_tokens(docs, 'NOUN', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5e: Print top proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 proper nouns from 1970:\n",
      "[('tilanus', 6), ('augustus', 4), ('kamer', 4), ('reces', 2), ('uitging', 2), ('particulier', 2), ('dettmeijer', 2), ('verheugt', 2), ('ermede', 2), ('instem', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1973:\n",
      "[('september', 4), ('syrië', 2), ('israël', 2), ('prinsjesdag', 2), ('nederland', 2), ('smit', 2), ('high', 2), ('wiegel', 2), ('hedenmiddag', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1974:\n",
      "[('vonhoff', 12), ('nederland', 4), ('levenssfeer', 2), ('leeuwen', 2), ('dienstverleningen', 2), ('januari', 2), ('november', 2), ('kleisieriee', 2), ('wolff', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1975:\n",
      "[('mevrou', 8), ('waaro', 6), ('kinderdagverblijven', 4), ('overblijfcentra', 4), ('vrou', 4), ('kamer', 4), ('adres', 4), ('verbrugh', 4), ('februari', 2), ('hetvrijwilligerswerk', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1976:\n",
      "[('kamer', 8), ('tweede', 6), ('november', 6), ('worrell', 6), ('oktober', 4), ('zweden', 2), ('frankrijk', 2), ('duitsland', 2), ('nederland', 2), ('evenhuis', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1977:\n",
      "[('tilanus', 4), ('parlement', 2), ('uitwerking', 2), ('geest', 2), ('tekst', 2), ('mertens', 2), ('mevrou', 2), ('donderdag', 2), ('november', 2), ('visie', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1978:\n",
      "[('worrell', 6), ('haas', 4), ('ongeschoold', 2), ('jong', 2), ('wouters', 2), ('schaapman', 2), ('nederland', 2), ('kamer', 2), ('wallis', 2), ('mevrou', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1979:\n",
      "[('nederland', 28), ('kamer', 12), ('wallis', 12), ('mevrou', 10), ('verkerk', 8), ('december', 8), ('vries', 8), ('tweede', 6), ('mertens', 4), ('worrell', 4)]\n",
      "\n",
      "\n",
      "These are the top 10 proper nouns from 1980:\n",
      "[('oskamp', 16), ('januari', 14), ('nederland', 14), ('vonhoff', 12), ('amsterdam', 10), ('kamer', 10), ('december', 10), ('oktober', 10), ('wallis', 8), ('meuleman', 8)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_top_tokens(docs, 'PROPN', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5f: Print top verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the top 10 verbs from 1970:\n",
      "[('afgevaardigde', 16), ('komen', 10), ('zeggen', 10), ('stellen', 10), ('lijkt', 8), ('gevraagd', 8), ('betreft', 8), ('weet', 8), ('gaat', 8), ('komt', 8)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1973:\n",
      "[('maakt', 4), ('staat', 4), ('gezegd', 4), ('gericht', 4), ('gerealiseerd', 4), ('wijden', 2), ('uitgerekend', 2), ('veroordeelt', 2), ('egypte', 2), ('sluiten', 2)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1974:\n",
      "[('afgevaardigde', 14), ('gesteld', 10), ('willen', 10), ('maken', 10), ('gemaakt', 8), ('zeggen', 8), ('gezegd', 8), ('gericht', 8), ('spreekt', 6), ('geraakt', 6)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1975:\n",
      "[('gaan', 28), ('betreft', 22), ('maken', 20), ('komen', 20), ('gaat', 20), ('zeggen', 18), ('barendregt', 18), ('willen', 16), ('gezegd', 16), ('gemaakt', 14)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1976:\n",
      "[('willen', 26), ('komen', 22), ('gezien', 20), ('stellen', 16), ('gebracht', 16), ('bewindslieden', 16), ('arbeidsmarkt', 12), ('zeggen', 12), ('vragen', 12), ('betreft', 10)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1977:\n",
      "[('gezegd', 10), ('staan', 6), ('dient', 6), ('gevraagd', 4), ('gezien', 4), ('gaan', 4), ('ruilen', 4), ('innemen', 4), ('deelneemt', 4), ('geldt', 4)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1978:\n",
      "[('gaat', 16), ('overwegende', 12), ('willen', 10), ('gebeurt', 10), ('maken', 10), ('krijgen', 8), ('werken', 8), ('blijft', 8), ('komen', 8), ('beperkte', 6)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1979:\n",
      "[('maken', 50), ('komen', 48), ('gevraagd', 44), ('willen', 42), ('gaan', 32), ('komt', 28), ('nemen', 28), ('gebracht', 28), ('gemaakt', 26), ('gaat', 26)]\n",
      "\n",
      "\n",
      "These are the top 10 verbs from 1980:\n",
      "[('komen', 58), ('gaan', 48), ('gaat', 42), ('komt', 40), ('betreft', 38), ('afgevaardigde', 34), ('brengen', 26), ('denken', 24), ('arbeidsmarkt', 22), ('blijkt', 22)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract_top_tokens(docs, 'VERB', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Print ALL words with a particular POS-tag (NOUN, ADJ, VERB, PRON, PROPN, SYM, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING: Only use this code if your dataset is manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print all words with a specific POS tag\n",
    "def print_all_words_by_pos(docs, pos_tag):\n",
    "    for year, doc in docs:\n",
    "        # Extract tokens with the desired POS tag\n",
    "        tokens = [token.text for token in doc if token.pos_ == pos_tag]\n",
    "        \n",
    "        # Print all tokens for the year\n",
    "        print(f'All {pos_tag.lower()}s in {year}:')\n",
    "        print(tokens)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_words_by_pos(docs, 'NOUN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create Named Entity list \n",
    "(doesn't work well for Dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lst = nlp.pipe_labels['ner']\n",
    "print(len(ner_lst))\n",
    "print(ner_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_list = ['EVENT', 'FAC', 'LAW', 'LOC', 'MONEY', 'ORG', 'PERSON', 'PRODUCT']\n",
    "\n",
    "for year, text in zip(df.index, df['text']):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print('Named entities in ' + year +':')\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in NER_list:\n",
    "            print(ent.text,  ent.label_)\n",
    "            print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: POS Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['']\n",
    "windows = [10] # add or change to smaller/larger window\n",
    "algorithms = ['likelihood', 'pmi'] # 'likelihood', 'pmi', 'raw_freq'\n",
    "coll_to_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter tokens by POS tags\n",
    "def filter_tokens_by_pos(doc, pos_tags):\n",
    "    return [token.text for token in doc if token.pos_ in pos_tags and not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_coll = outdir / f'{save_corpus(dataset)}_pos-collocations'\n",
    "outdir_coll.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "for term in search_terms:\n",
    "    for window in windows:\n",
    "        for algorithm in algorithms:\n",
    "            outfile_coll = f'{term}_{algorithm}_pos-collocations_{window}.txt'\n",
    "            outpath_coll = outdir_coll / outfile_coll\n",
    "            \n",
    "            with open(outpath_coll, 'a') as f:\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset))\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset), file=f)\n",
    "                for year, doc in docs:\n",
    "                    # Filter tokens by desired POS types (e.g., nouns and proper nouns)\n",
    "                    filtered_tokens = filter_tokens_by_pos(doc, {'ADJ'})\n",
    "                    \n",
    "                    # Tokenize the filtered tokens\n",
    "                    tokens = WordPunctTokenizer().tokenize(' '.join(filtered_tokens))\n",
    "\n",
    "                    bigram_measures = collocations.BigramAssocMeasures()\n",
    "                    word_fd = FreqDist(tokens)\n",
    "                    bigram_fd = FreqDist(bigrams(tokens))\n",
    "                    finder = BigramCollocationFinder(word_fd, bigram_fd, window_size=window)\n",
    "\n",
    "                    #preprocessing: remove short words and stop words (see above) if only relevant for collocations\n",
    "                    #finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in stopwords)\n",
    "        \n",
    "                    if algorithm == 'likelihood': \n",
    "                        scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "                    elif algorithm == 'pmi': \n",
    "                        scored = finder.score_ngrams(bigram_measures.pmi) \n",
    "                    else: \n",
    "                        scored = finder.score_ngrams(bigram_measures.raw_freq) \n",
    "                  \n",
    "                    # Group bigrams by first word in bigram                                       \n",
    "                    prefix_keys = defaultdict(list)\n",
    "                    for key, scores in scored:\n",
    "                        prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "                    # Sort keyed bigrams by strongest association                                  \n",
    "                    for key in prefix_keys:\n",
    "                        prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "                    # Print top collocations of term.\n",
    "                    print(str(year) + ':')\n",
    "                    print(str(year) + ':', file=f)\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n')\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n', file=f)\n",
    "                    print('\\n')\n",
    "                    print('\\n', file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
