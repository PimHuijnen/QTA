{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 6\n",
    "\n",
    "## Part-of-speech, Named entity recognition _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download spacy model if needed (see https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.cli.download import download\n",
    "# download(model=\"nl_core_news_sm\") # en_core_web_sm is the standard model for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "nlp.max_length = 2000000\n",
    "nlp.Defaults.stop_words |= {'the'} # add words as 'word', 'word', 'word'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define in- and out-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = r'/path/to/indir/'\n",
    "outdir = r'/path/to/outdir/'\n",
    "os.makedirs(os.path.dirname(outdir), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "dataset = 'dataset' # give a name to your dataset for outfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe \n",
    "Df with the texts in \"text\" column and the file name (=date) in \"file_name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "# list all files in a given directory\n",
    "files = os.listdir(indir)\n",
    "files = [f for f in files if not f.startswith('.')]\n",
    "\n",
    "for infile in files:\n",
    "    # avoid opening files such as .DS_Store\n",
    "    if infile.startswith('.'):\n",
    "        continue\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(indir+infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "        results[\"year\"].append(infile[:-4])\n",
    "        results[\"text\"].append(text)\n",
    "        \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn \"file_name\" column into datetime and set as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"year\"], format =\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"date\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.index.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To print n most common words with a particular POS-tag (resp. adjectives, verbs, proper nouns and nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df['text']): ### zip om door meerdere kolommen te itereren\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # adj tokens that arent stop words or punctuations\n",
    "    adjs = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"ADJ\")]\n",
    "\n",
    "    print('These are the top', str(n), 'adjectives from', str(year),':')\n",
    "\n",
    "    # five most common adj tokens\n",
    "    adj_freq = Counter(adjs)\n",
    "    common_adjs = adj_freq.most_common(n)\n",
    "    print(common_adjs)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df['text']):\n",
    "    doc = nlp(text) \n",
    "    \n",
    "    # verb tokens that arent stop words or punctuations\n",
    "    verbs = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"VERB\")]\n",
    "\n",
    "    print('These are the top', str(n), 'verbs from', str(year),':')\n",
    "    \n",
    "    # five most common verb tokens\n",
    "    verbs_freq = Counter(verbs)\n",
    "    common_verbs = verbs_freq.most_common(n)\n",
    "    print(common_verbs)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df['text']):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # proper noun tokens that arent stop words or punctuations\n",
    "    pnouns = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"PROPN\")]\n",
    "\n",
    "    print('These are the top', str(n), 'proper nouns from', str(year),':')\n",
    "    \n",
    "    # five most common proper noun tokens\n",
    "    pnoun_freq = Counter(pnouns)\n",
    "    common_pnouns = pnoun_freq.most_common(n)\n",
    "    print(common_pnouns)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df['text']):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # noun tokens that arent stop words or punctuations\n",
    "    nouns = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]\n",
    "\n",
    "    print('These are the top', str(n), 'nouns from', str(year),':')\n",
    "    \n",
    "    # five most common noun tokens\n",
    "    noun_freq = Counter(nouns)\n",
    "    common_nouns = noun_freq.most_common(n)\n",
    "    print(common_nouns)\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To print ALL words with a particular POS-tag (NOUN, ADJ, VERB, PRON, PROPN, SYM, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"SYM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df['text']):\n",
    "    doc = nlp(text)\n",
    "    print([(token.text, token.tag_, token.pos_) for token in doc if token.pos_ == 'SYM']) # change POS-tag here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Named Entity list \n",
    "(doesn't work well for Dutch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lst = nlp.pipe_labels['ner']\n",
    "print(len(ner_lst))\n",
    "print(ner_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_list = ['EVENT', 'FAC', 'LAW', 'LOC', 'MONEY', 'ORG', 'PERSON', 'PRODUCT']\n",
    "\n",
    "for year, text in zip(df.year, df['text']):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    print('Named entities in ' + year +':')\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in NER_list:\n",
    "            print(ent.text,  ent.label_)\n",
    "            print('\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
