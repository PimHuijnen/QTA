{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da66f75",
   "metadata": {},
   "source": [
    "# Doing Things with Text 3a: Import and clean multiple text files\n",
    "\n",
    "This notebook introduces the automatic cleaning and saving of multiple text files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e6a00",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a library for working with text. To use it, you'll need to download some additional language data the first time you use NLTK. Run the following cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb2e1-ade3-4647-9b3f-400e6a548835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required packages\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e0249-2479-4798-9252-a8eedc415d2d",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Packages\n",
    "\n",
    "Here, we're loading a few packages to help with text cleaning:\n",
    "- `BeautifulSoup`: To clean up HTML text.\n",
    "- `os`: Helps with interacting with the operating system, such as managing file paths and directories.\n",
    "- `re`: For regular expressions (patterns used for finding and cleaning text).\n",
    "- `nltk.tokenize`: For splitting text into individual words.\n",
    "- `nltk.corpus.stopwords`: A collection of common words like 'the', 'and', 'is', which are often removed in analysis.\n",
    "- `matplotlib.pyplot`: Allows for creating visualizations like charts and graphs to represent data visually.\n",
    "-  `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b0731",
   "metadata": {},
   "source": [
    "### Step 3: Define Input and Output Paths\n",
    "\n",
    "Define where your text file is located (input) and where you want to save your processed text (output). You will use `os.path.join()` to define your paths. This approach is cross-platform, meaning it will work on Windows, macOS, and Linux.\n",
    "\n",
    "Replace 'path', 'to', 'your', 'input', 'folder' with the actual paths to your files. It is not necessary for the output folder to exist. If it doesn't, this code will create it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = os.path.join('path', 'to', 'your', 'input', 'folder')\n",
    "outdir = os.path.join('path', 'to', 'your', 'output', 'folder')\n",
    "os.makedirs(outdir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a653-babf-40e4-b2d2-dbe5da7b91d1",
   "metadata": {},
   "source": [
    "### Step 3: Load text documents, clean, and write to outdir\n",
    "\n",
    "Everything that we did in notebook 1 step by step is comprimised in the next code block. First, it creates some variables that we need, next it starts a `for` loop that loops through the directory we named 'indir', opens files one by one, cleans them, and saves them to the directory named 'outdir'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c950093-f50d-427c-82e0-360c60897583",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "token_counts_before = []\n",
    "token_counts_after = []\n",
    "all_cleaned_text = []\n",
    "cleaned_data = []\n",
    "\n",
    "for filename in os.listdir(indir):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(indir, filename)\n",
    "        with open(file_path, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens_before = word_tokenize(text)\n",
    "        token_counts_before.append(len(tokens_before))\n",
    "\n",
    "        # Cleaning steps\n",
    "        text = text.lower()\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        text = re.sub('[^a-z\\\\s]', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        token_counts_after.append(len(tokens))\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        all_cleaned_text.append(cleaned_text)\n",
    "        cleaned_data.append({'filename': filename, 'cleaned_text': cleaned_text})\n",
    "\n",
    "        output_file_path = os.path.join(outdir, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf8') as f:\n",
    "            f.write(cleaned_text)\n",
    "        file_names.append(filename)\n",
    "        print(f'Processed and saved: {filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9f174",
   "metadata": {},
   "source": [
    "### Step 4: Generate Bar Chart of Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(file_names))\n",
    "\n",
    "plt.bar(index, token_counts_before, bar_width, label='Before Cleaning')\n",
    "plt.bar([i + bar_width for i in index], token_counts_after, bar_width, label='After Cleaning')\n",
    "\n",
    "plt.xlabel('Files')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.title('Token Counts Before and After Cleaning')\n",
    "plt.xticks([i + bar_width / 2 for i in index], file_names, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07539374",
   "metadata": {},
   "source": [
    "### (Optional) Step 5: Save All Cleaned Text to a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73682af",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_output_file = os.path.join(outdir, 'cleaned_text_%s.txt' %(dataset))\n",
    "with open(merged_output_file, 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_cleaned_text))\n",
    "print('Merged cleaned text saved to:', merged_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653358c",
   "metadata": {},
   "source": [
    "### (Optional) Step 6: Save Cleaned Text in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26923e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.DataFrame(cleaned_data)\n",
    "csv_output_file = os.path.join(outdir, 'cleaned_text_%s.csv' %(dataset)) \n",
    "df_cleaned.to_csv(csv_output_file, index=False, encoding='utf8')\n",
    "print('All cleaned text saved to CSV:', csv_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
