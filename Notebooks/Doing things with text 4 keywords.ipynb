{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 4\n",
    "\n",
    "## Word frequency, n-grams, collocations _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk.util import everygrams\n",
    "from nltk import collocations\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk import WordPunctTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = '/path/to/indir/'\n",
    "outdir = '/path/to/outdir/'\n",
    "os.makedirs(os.path.dirname(outdir), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe \n",
    "Df with the texts in \"text\" column and the file name (=date) in \"file_name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "\n",
    "# list all files in a given directory\n",
    "files = os.listdir(indir)\n",
    "#files = [f for f in files if not f.startswith('.')]\n",
    "\n",
    "for infile in files:\n",
    "    # avoid opening files such as .DS_Store\n",
    "    if infile.startswith('.'):\n",
    "        continue\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(indir+infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "        results[\"year\"].append(infile[:-4])\n",
    "        results[\"text\"].append(text)\n",
    "        \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn \"year\" column into datetime and set as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"year\"], format =\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"date\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.index.strftime('%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional preprocessing (only run if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [] # add words as 'word', 'word', 'word', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [[x for x in text.split(' ') if len(x) >= minimum_word_length and x not in stopwords] \n",
    "              for text in df['text']]\n",
    "df['text'] = df['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and plot word frequencies per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    words = string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,8))\n",
    "\n",
    "plt.bar(df.year, df['num_words'])\n",
    "plt.ylabel('words')\n",
    "plt.xlabel('date')\n",
    "plt.title(\"Total number of words per text in %s\" %(dataset))\n",
    "plt.savefig(outdir + 'total_words_%s.png' %(dataset))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and visualizing (ngram) strings in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(list):\n",
    "    string = '_'.join(list)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words_freq = [] # add words as 'word', 'word', 'word', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,8))\n",
    "\n",
    "for search_word in search_words_freq:\n",
    "    df[search_word + '_rel'] = df.text.str.count(pat=search_word + '??') / df.num_words\n",
    "    plt.scatter(df.year, df[search_word + '_rel'], label=search_word)\n",
    "    with open(str(search_word) + '_freq.txt', 'a') as outfile:\n",
    "        print(df.text.str.count(pat=search_word + '??') / df.num_words, file=outfile)\n",
    "\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('year')\n",
    "plt.title(\"Keyword trends in %s\" %(dataset))\n",
    "plt.legend()\n",
    "plt.minorticks_on()\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(f'{outdir}{to_string(search_words_freq)}_{dataset}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find collocations:\n",
    "\n",
    "From: https://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words_coll = [] # add words as 'word', 'word', 'word', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [10, 20]\n",
    "algorithms = ['likelihood', 'pmi', 'raw_freq']\n",
    "\n",
    "for window in windows:\n",
    "    for algorithm in algorithms:\n",
    "        for year, doc in zip(df.year, df.text):\n",
    "            tokens = WordPunctTokenizer().tokenize(doc)\n",
    "\n",
    "            bigram_measures = collocations.BigramAssocMeasures()\n",
    "            word_fd = FreqDist(tokens)\n",
    "            bigram_fd = FreqDist(bigrams(tokens))\n",
    "            finder = BigramCollocationFinder(word_fd, bigram_fd, window_size=window)\n",
    "\n",
    "            #preprocessing: remove short words and stop words (see above) if only relevant for collocations\n",
    "            #finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in stopwords)\n",
    "        \n",
    "            if algorithm == 'likelihood': \n",
    "                scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "            elif algorithm == 'pmi': \n",
    "                scored = finder.score_ngrams(bigram_measures.pmi) \n",
    "            else: \n",
    "                scored = finder.score_ngrams(bigram_measures.raw_freq) \n",
    "\n",
    "            # Group bigrams by first word in bigram.                                        \n",
    "            prefix_keys = defaultdict(list)\n",
    "            for key, scores in scored:\n",
    "                prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "            # Sort keyed bigrams by strongest association.                                  \n",
    "            for key in prefix_keys:\n",
    "                prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "            # Print top collocations of search_terms\n",
    "\n",
    "            for term in search_words_coll:\n",
    "                outfp = f'{term}_collocations_{algorithm}_{window}.txt'\n",
    "                output_fp = os.path.join(outdir, dataset + '_' + term + '_coll', outfp)\n",
    "                os.makedirs(os.path.dirname(output_fp), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "                with open(output_fp, 'a') as outfile:\n",
    "                    print('Top collocations of ' + term + ' in ' + str(year) + ':')\n",
    "                    print('Top collocations of ' + term + ' in ' + str(year) + ':', file=outfile)\n",
    "                    print(*prefix_keys[term][:n], sep='\\n')\n",
    "                    print(*prefix_keys[term][:n], sep='\\n', file=outfile)\n",
    "                    print('\\n')\n",
    "                    print('\\n', file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and save to outdir the top n ngrams per dataframe row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define length of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print and write to disk the n most common ngrams of this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df.text):\n",
    "       \n",
    "    grams = ngrams(text.split(), ngram)\n",
    "    grams_freq = Counter(grams)\n",
    "    top_grams = grams_freq.most_common(n) # for n see above\n",
    "    \n",
    "    outfp = f'{dataset}_{year}_{ngram}_grams.txt'\n",
    "    output_fp = os.path.join(outdir, dataset + '_%s_grams' %(ngram), outfp)\n",
    "    os.makedirs(os.path.dirname(output_fp), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "    with open(output_fp, 'a') as outfile:\n",
    "        print('Top ' + str(n) + ' ' + str(ngram) + '-grams in ' + year + ':')\n",
    "        print('Top ' + str(n) + ' ' + str(ngram) + '-grams in ' + year + ':', file=outfile)\n",
    "        print(*top_grams, sep='\\n')\n",
    "        print(*top_grams, sep='\\n', file=outfile)\n",
    "        print('\\n')\n",
    "        print('\\n', file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print and write to disk the most common ngrams of this length beginning or ending with a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams_begin(year, word):\n",
    "    outfp = f'{dataset}_{word}_{year}_{ngram}_grams_begin.txt'\n",
    "    outfolder = os.path.join(outdir, dataset + '_%s_grams' %(ngram), outfp) # same folder as in previous step\n",
    "    os.makedirs(os.path.dirname(outfolder), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "    with open(outfolder, 'a') as outfile:\n",
    "        print(f'Top', str(ngram) + '-grams starting with \\'' + word + '\\' in', year, ':')\n",
    "        print(f'Top', str(ngram) + '-grams starting with \\'' + word + '\\' in', year, ':', file=outfile)\n",
    "        for item in top_grams:\n",
    "            if item[0][0] == word:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=outfile)            \n",
    "            \n",
    "def top_ngrams_end(top_grams, year, word):\n",
    "    end = ngram - 1\n",
    "    outfp = f'{dataset}_{word}_{year}_{ngram}_grams_end.txt'\n",
    "    outfolder = os.path.join(outdir, dataset + '_%s_grams' %(ngram), outfp) # same folder as in previous step\n",
    "    os.makedirs(os.path.dirname(outfolder), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "    with open(outfolder, 'a') as outfile:\n",
    "        print(f'Top', str(ngram) + '-grams ending with \\'' + word + '\\' in', year, ':')\n",
    "        print(f'Top', str(ngram) + '-grams ending with \\'' + word + '\\' in', year, ':', file=outfile)\n",
    "        for item in top_grams:\n",
    "            if item[0][end] == word:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=outfile)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginword = # type as 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df.text):\n",
    "       \n",
    "    grams = ngrams(text.split(), ngram)\n",
    "    grams_freq = Counter(grams)\n",
    "    top_grams = grams_freq.most_common(1000)\n",
    "    \n",
    "    top_ngrams_begin(year, beginword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endword =  # type as 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, text in zip(df.year, df.text):\n",
    "       \n",
    "    grams = ngrams(text.split(), ngram)\n",
    "    grams_freq = Counter(grams)\n",
    "    top_grams = grams_freq.most_common(1000)\n",
    "\n",
    "    top_ngrams_end(top_grams, year, endword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
