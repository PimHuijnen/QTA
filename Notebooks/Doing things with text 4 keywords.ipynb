{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 4\n",
    "\n",
    "## Word frequency, n-grams, collocations _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk.util import everygrams\n",
    "from nltk import collocations\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk import WordPunctTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.home()\n",
    "indir = p / 'folder' / 'folder'\n",
    "outpath = p / 'folder' / 'folder'\n",
    "outdir = str(outpath)+'/'\n",
    "os.makedirs(os.path.dirname(outdir), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out\n",
    "\n",
    "def to_string(list):\n",
    "    string = '_'.join(list)\n",
    "    return string\n",
    "\n",
    "def to_title(words):\n",
    "    if not words:\n",
    "        return ''\n",
    "    elif len(words) == 1:\n",
    "        return f\"'{words[0]}'\"\n",
    "    else:\n",
    "        formatted_list = [f\"'{word}'\" for word in words[:-1]]\n",
    "        return ', '.join(formatted_list) + f\" and '{words[-1]}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe \n",
    "Df with the texts in \"text\" column and the file name (=date) in \"file_name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "\n",
    "# list all files in a given directory\n",
    "files = os.listdir(indir)\n",
    "#files = [f for f in files if not f.startswith('.')]\n",
    "\n",
    "for infile in files:\n",
    "    # avoid opening files such as .DS_Store\n",
    "    if infile.startswith('.'):\n",
    "        continue\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(str(indir) + '/' + infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "        results[\"year\"].append(infile[:-4])\n",
    "        results[\"text\"].append(text)\n",
    "        \n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn \"year\" column into datetime and set as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"year\")\n",
    "df.index = pd.to_datetime(df.index, format =\"%Y\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional preprocessing (only run if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['word1', 'word2', 'word3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [[x for x in text.split(' ') if len(x) >= minimum_word_length and x not in stopwords] \n",
    "              for text in df['text']]\n",
    "df['text'] = df['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count and plot word frequencies per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    words = string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath_counts = outdir + f'total_words_{save_corpus(dataset)}.png'\n",
    "\n",
    "fig = plt.figure(figsize = (15,8))\n",
    "\n",
    "plt.bar(df.index.year, df['num_words'])\n",
    "plt.ylabel('words')\n",
    "plt.xlabel('date')\n",
    "plt.xticks(df.index.year, rotation=45)\n",
    "plt.title(\"Total number of words per year in %s\" %(dataset))\n",
    "plt.savefig(outpath_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and visualizing (ngram) strings in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = ['word', 'word', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_freq = f'{to_string(search_words)}_{save_corpus(dataset)}_freq.png'\n",
    "outdir_freq = os.path.join(outdir, save_corpus(dataset) + '_keyword_trends/')\n",
    "outpath_freq = outdir_freq + outfile_freq\n",
    "os.makedirs(os.path.dirname(outpath_freq), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "fig = plt.figure(figsize = (15,8))\n",
    "\n",
    "for search_word in search_words:\n",
    "    df[search_word + '_rel'] = df.text.str.count(pat=search_word + '??') / df.num_words\n",
    "    plt.plot(df.index.year, df[search_word + '_rel'], label=search_word)\n",
    "    with open(outdir_freq + search_word + '_' + save_corpus(dataset) + '_freq.txt', 'a') as f:\n",
    "        print('Relative frequency of \\'%s\\' in %s\\n' %(search_word, dataset), file=f)\n",
    "        print(df.text.str.count(pat=search_word + '??') / df.num_words, file=f)\n",
    "\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('year')\n",
    "plt.title(\"Keyword trends in %s\" %(dataset))\n",
    "plt.legend()\n",
    "plt.xticks(df.index.year, rotation=45)\n",
    "plt.savefig(outpath_freq)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and printing word endings in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending = 'ing'\n",
    "min_freq_end = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_end = 'top_endings_%s_%s.txt'%(ending, save_corpus(dataset))\n",
    "outpath_end = os.path.join(outdir, save_corpus(dataset) + '_endings_beginnings', outfile_end)\n",
    "os.makedirs(os.path.dirname(outpath_end), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "with open(outpath_end, 'a') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        print(str(index)[:4])\n",
    "        print(str(index)[:4], file=f)\n",
    "        word_counts = Counter(row['text'].split())\n",
    "        word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "        for word, count in word_counts.items():\n",
    "            if word.endswith(ending) and count >= min_freq_end:\n",
    "                print(word + \": \", count)\n",
    "                print(word + \": \", count, file=f)\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and printing word beginnings in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 'pre'\n",
    "min_freq_begin = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_begin = 'top_beginnings_%s_%s.txt'%(begin, save_corpus(dataset))\n",
    "outpath_begin = os.path.join(outdir, save_corpus(dataset) + '_endings_beginnings', outfile_begin)\n",
    "os.makedirs(os.path.dirname(outpath_begin), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "with open(outpath_begin, 'a') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        print(str(index)[:4])\n",
    "        print(str(index)[:4], file=f)\n",
    "        word_counts = Counter(row['text'].split())\n",
    "        word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "        for word, count in word_counts.items():\n",
    "            if word.startswith(begin) and count >= min_freq_begin:\n",
    "                print(word, \": \", count)\n",
    "                print(word, \": \", count, file=f)\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find collocations:\n",
    "\n",
    "From: https://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['word', 'word']\n",
    "windows = [10] # add or change to smaller/larger window\n",
    "algorithms = ['likelihood', 'pmi'] # 'likelihood', 'pmi', 'raw_freq'\n",
    "coll_to_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in search_terms:\n",
    "    for window in windows:\n",
    "        for algorithm in algorithms:\n",
    "            outfile_coll = f'{term}_{algorithm}_collocations_{window}.txt'\n",
    "            outpath_coll = os.path.join(outdir, save_corpus(dataset) + '_collocations', outfile_coll)\n",
    "            os.makedirs(os.path.dirname(outpath_coll), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "            \n",
    "            with open(outpath_coll, 'a') as f:\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset))\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset), file=f)\n",
    "                for year, doc in zip(df.index.year, df.text):\n",
    "                    tokens = WordPunctTokenizer().tokenize(doc)\n",
    "            \n",
    "                    bigram_measures = collocations.BigramAssocMeasures()\n",
    "                    word_fd = FreqDist(tokens)\n",
    "                    bigram_fd = FreqDist(bigrams(tokens))\n",
    "                    finder = BigramCollocationFinder(word_fd, bigram_fd, window_size=window)\n",
    "\n",
    "                    #preprocessing: remove short words and stop words (see above) if only relevant for collocations\n",
    "                    #finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in stopwords)\n",
    "        \n",
    "                    if algorithm == 'likelihood': \n",
    "                        scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "                    elif algorithm == 'pmi': \n",
    "                        scored = finder.score_ngrams(bigram_measures.pmi) \n",
    "                    else: \n",
    "                        scored = finder.score_ngrams(bigram_measures.raw_freq) \n",
    "\n",
    "                    # Group bigrams by first word in bigram                                       \n",
    "                    prefix_keys = defaultdict(list)\n",
    "                    for key, scores in scored:\n",
    "                        prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "                    # Sort keyed bigrams by strongest association                                  \n",
    "                    for key in prefix_keys:\n",
    "                        prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "                    # Print top collocations of term.\n",
    "                    print(str(year) + ':')\n",
    "                    print(str(year) + ':', file=f)\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n')\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n', file=f)\n",
    "                    print('\\n')\n",
    "                    print('\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and save to outdir the top n ngrams per dataframe row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define length of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 2\n",
    "words_to_print = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print and write to disk the n most common ngrams of this length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_ngram = f'{save_corpus(dataset)}_mostcommon_{ngram}_grams.txt'\n",
    "outpath_ngram = os.path.join(outdir, save_corpus(dataset) + '_ngrams', outfile_ngram)\n",
    "os.makedirs(os.path.dirname(outpath_ngram), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "with open(outpath_ngram, 'a') as f:\n",
    "\n",
    "    print('Top ' + str(words_to_print) + ' ' + str(ngram) + '-grams in ' + dataset + ':\\n')\n",
    "    print('Top ' + str(words_to_print) + ' ' + str(ngram) + '-grams in ' + dataset + ':\\n', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print) # for n see above\n",
    "        \n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        print(*top_grams, sep='\\n')\n",
    "        print(*top_grams, sep='\\n', file=f)\n",
    "        print('\\n')\n",
    "        print('\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print and write to disk the most common ngrams of this length beginning or ending with a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginword = 'word' # type as 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_ngramend = f'{save_corpus(dataset)}_{beginword}_{ngram}_grams_begin.txt'\n",
    "outpath_ngramend = os.path.join(outdir, save_corpus(dataset) + '_ngrams', outfile_ngramend)\n",
    "os.makedirs(os.path.dirname(outpath_ngramend), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "with open(outpath_ngramend, 'a') as f:\n",
    "    print(f'Top', str(ngram) + '-grams beginning with \\'' + beginword + '\\' in ' + dataset + ':\\n')\n",
    "    print(f'Top', str(ngram) + '-grams beginning with \\'' + beginword + '\\' in ' + dataset + ':\\n', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print)\n",
    "        for item in top_grams:\n",
    "            if item[0][0] == beginword:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endword = 'word' # type as 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = ngram - 1\n",
    "\n",
    "outfile_ngrambegin = f'{save_corpus(dataset)}_{endword}_{ngram}_grams_end.txt'\n",
    "outpath_ngrambegin = os.path.join(outdir, save_corpus(dataset) + '_ngrams', outfile_ngrambegin)\n",
    "os.makedirs(os.path.dirname(outpath_ngrambegin), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "with open(outpath_ngrambegin, 'a') as f:\n",
    "    print(f'Top', str(ngram) + '-grams ending with \\'' + endword + '\\' in ' + dataset + ':')\n",
    "    print(f'Top', str(ngram) + '-grams ending with \\'' + endword + '\\' in ' + dataset + ':', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print)\n",
    "        for item in top_grams:\n",
    "            if item[0][end] == endword:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=f)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
