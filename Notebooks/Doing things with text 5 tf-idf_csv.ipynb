{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 5\n",
    "\n",
    "## TF-IDF on multiple texts _for preprocessed texts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `collections.defaultdict`: Creates dictionaries with default values for missing keys.\n",
    "- `sklearn.TfidfVectorizer`: Transforms text data into TF-IDF features for text analysis and machine learning.\n",
    "- `sklearn.CountVectorizer`: Converts text data into a matrix of token counts for analysis.\n",
    "- `sklearn.cosine_similarity`: Computes the cosine similarity between vectors for similarity analysis.\n",
    "- `sklearn.linear_kernel`: Calculates the linear kernel (dot product) for vector similarity or SVMs.\n",
    "- `pandas`: Provides data structures and tools for data manipulation and analysis.\n",
    "- `matplotlib.pyplot`: Creates static, animated, and interactive visualizations in Python.\n",
    "- `seaborn`: Builds on matplotlib for enhanced statistical data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define input and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/outdir/')\n",
    "outdir.mkdir(parents=True, exist_ok=True) # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.csv\"))\n",
    "\n",
    "dataset = 'dataset' # give a name to your dataset for outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in allfiles:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the data\n",
    "\n",
    "Stores al text files from indir as strings in a list input_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "for file in allfiles:\n",
    "    try:\n",
    "        # Load the CSV into a DataFrame\n",
    "        df_infile = pd.read_csv(file, sep='\\t')\n",
    "        \n",
    "        # Ensure 'date' and 'text' are available\n",
    "        if 'date' in df_infile.columns:\n",
    "            date_column = df_infile['date']\n",
    "        else:\n",
    "            date_column = df_infile.index  # Use index if 'date' column doesn't exist\n",
    "\n",
    "        # Append the data to results\n",
    "        for date, text in zip(date_column, df_infile['text_clean_str']):\n",
    "            results[\"date\"].append(date)\n",
    "            results[\"text\"].append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3b: Set 'date' column to datetime index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"date\")\n",
    "df.index = pd.to_datetime(df.index, format =\"%d-%m-%Y\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3c: Create lists for text content and index for tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the year\n",
    "df['year'] = df.index.year\n",
    "\n",
    "# 2. Group by year and concatenate text (this will group by year, not date)\n",
    "grouped = df.groupby('year')['text'].apply(' '.join)\n",
    "\n",
    "# 3. Create file_names (list of years) and all_docs (concatenated text for each year)\n",
    "file_names = grouped.index.tolist()\n",
    "all_docs = grouped.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tf-idf analysis\n",
    "\n",
    "From: https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf\n",
    "See also: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=.65, min_df=1, stop_words=None, use_idf=True, norm=None)\n",
    "transformed_documents = vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_documents_as_array = transformed_documents.toarray()\n",
    "# use this line of code to verify that the numpy array represents the same number of documents that we have in the file list\n",
    "len(file_names) == len(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformed_documents_as_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_tfidf_csv = outdir / f'{save_corpus(dataset)}_tfidf_csv/'\n",
    "outdir_tfidf_csv.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outdir_tfidf_png = outdir / f'{save_corpus(dataset)}_tfidf_png/'\n",
    "outdir_tfidf_png.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "# Set number of n top terms\n",
    "n = 20\n",
    "\n",
    "# Loop through each document\n",
    "for counter, doc in enumerate(transformed_documents_as_array):\n",
    "    # Get the original filename without the extension\n",
    "    original_filename = file_names[counter]\n",
    "\n",
    "    # Construct a dataframe\n",
    "    tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "    one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, \n",
    "                                              columns=['term', 'score']).sort_values(by='score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Output to CSV\n",
    "    one_doc_as_df.to_csv(outdir_tfidf_csv / f'{original_filename}_tfidf.csv')\n",
    "\n",
    "    # Select top terms\n",
    "    top_terms = one_doc_as_df[:n]\n",
    "    \n",
    "    # Create a separate figure for each document\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(top_terms['term'], top_terms['score'])\n",
    "    plt.xlabel('Top terms')\n",
    "    plt.ylabel('tf-idf score')\n",
    "    plt.title(f'Top {n} terms with highest tf-idf scores in {original_filename}')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.savefig(outdir_tfidf_png / f\"{original_filename}_tfidf.png\", dpi=300) # Save individual chart with the original filename\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer()\n",
    "count_matrix = vectorizer2.fit_transform(all_docs)\n",
    "cosine_sim = cosine_similarity(count_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn cosine_sim into pandas dataframe to visualize in heatmap. Name columns and index after correct year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_docs = pd.DataFrame(cosine_sim, columns = file_names)\n",
    "df_all_docs.index = df_all_docs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_docs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "sns.heatmap(df_all_docs.corr(), square=True, cmap='RdYlGn', ax=ax)\n",
    "plt.title('Heatmap of Cosine Similarity scores')\n",
    "plt.savefig(outdir / f'{dataset}_heatmap.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
