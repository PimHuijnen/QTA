{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Things with Text 3c: Statistics on multiple text documents\n",
    "\n",
    "This notebook provides some basic statistics of the texts in one or more csv's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install packages (only the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a library for working with text. To use it, you'll need to download some additional language data the first time you use NLTK. Run the following cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required packages\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Packages\n",
    "\n",
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `csv`: Provides functionality for reading from and writing to CSV (Comma-Separated Values) files\n",
    "- `tqdm.tqdm`: Provides a progress bar for loops and iterable tasks, helping to track the progress of operations in real-time.\n",
    "- `matplotlib.pyplot`: Allows for creating visualizations like charts and graphs to represent data visually.\n",
    "- `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets.\n",
    "- `collections.Counter`: A specialized dictionary that counts the occurrences of elements in an iterable.\n",
    "- `wordcloud`: Creates a word cloud visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Input and Output Paths\n",
    "\n",
    "Define where your text file is located (indir) and where you want to save your processed text (outdir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/outdir/')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.csv\"))\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check what's in 'allfiles':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in allfiles:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 check what the data structure of csv's looks like (change 'file.csv' for one of the actual files in indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(str(indir) + '/' + 'file.csv', sep='\\t') # most common separators are ';' or ',' or '\\t'\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['year', 'text'] # change according to the column headers in your csv above\n",
    "index_col = 'year' # preferably, the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(dataset):\n",
    "    dataset_out = dataset.replace(\" \", \"_\").lower()\n",
    "    return dataset_out\n",
    "\n",
    "def remove_user_defined_stopword_list(words):\n",
    "    \"\"\" Given a hardcoded list of words and stop words, remove stop words \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in custom_words:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import csv's as df (with df['text']) as the text column), merge into one large dataframe called 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Use a list to collect DataFrames\n",
    "\n",
    "for file in tqdm(allfiles):\n",
    "    df = pd.read_csv(file, sep=\"\\t\", usecols=cols_to_keep, index_col=index_col)\n",
    "    data.append(df)  # Append the DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames at once\n",
    "data = pd.concat(data, axis=0, ignore_index=False)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) step 4a: group rows in dataframe 'data' by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = pd.to_datetime(data.index, format =\"%Y-%m-%d\") # format depends on format in index (date) column. See https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n",
    "data = data.sort_index()\n",
    "data = data.groupby(data.index.year).sum(numeric_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Make single lists and strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn text column in data into big list 'input_as_list'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: 'text' column is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_as_list = [item for sublist in data['text'] for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: 'text' column is a string of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_as_list = [word for text in data['text'] for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_as_list[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn text data into big string 'input_as_string'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: 'text' column is a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_as_string = \" \".join(word for sublist in data['text'] for word in sublist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: 'text' column is a string of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_as_string = \" \".join(data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_as_string[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Step 5c: User defined stopwords (for wordcloud and Counter). Change if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = ['het', 'van', 'een', 'dat', 'zijn'] ### add words as list: 'word', 'word', 'word', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove custom_words from input_as_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_as_list = remove_user_defined_stopword_list(input_as_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculate basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our tokenized text, we can now calculate some basic statistics, such as the number of unique words (types)\n",
    "and the total number of words (tokens).\n",
    "\n",
    "word_counts_total below is a counter object that counts the frequency for each of the words in input_as_list. It feeds the bar chart below. Words that need removed from the bar chart can be put in the custom stopword list custom_words above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique words\n",
    "word_counts_total = Counter(input_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of tokens in %s is: %s\"%(dataset, len(input_as_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of types in %s is: %s\" %(dataset, len(word_counts_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate lexical diversity by dividing number of types by number of tokens (= type token ratio, or TTR)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The type token ratio of {dataset} is: {round(len(word_counts_total)/len(input_as_list)*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Visualize most common words in a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_top_words = 20 # set number of most common words to print/plot\n",
    "most_common_total = word_counts_total.most_common(number_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### From https://stackoverflow.com/questions/63018726/counter-and-plot-the-most-common-word-in-a-text ####\n",
    "\n",
    "y = [count for word, count in most_common_total]\n",
    "x = [word for word, count in most_common_total]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "plt.bar(x, y, color='crimson')\n",
    "plt.title(\"Most common terms\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.xlabel(\"Terms\")\n",
    "plt.rc('xtick',labelsize=12)\n",
    "plt.rc('ytick',labelsize=12)\n",
    "#plt.yscale('log') # optionally set a log scale for the y-axis\n",
    "plt.xticks(rotation=45)\n",
    "for i, (word, count) in enumerate(most_common_total):\n",
    "    plt.text(i, count, f' {count} ', rotation=45, size=16,\n",
    "             ha='center', va='top' if i < 10 else 'bottom', color='white' if i < 10 else 'black')\n",
    "plt.xlim(-0.6, len(x)-0.4) # optionally set tighter x lims\n",
    "plt.tight_layout() # change the whitespace such that all labels fit nicely\n",
    "plt.savefig(outdir / f'{save_corpus(dataset)}_most_common.png', dpi=200, bbox_inches='tight') # change filename as wished\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Visualize most common words in a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate wordcloud**\n",
    "\n",
    "A word cloud visualizes word frequency, where the size of each word indicates its frequency. You can customize the background color and colormap of the word cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud\n",
    "text_cloud = WordCloud(background_color='white', stopwords=custom_words).generate(input_as_string)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.imshow(text_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig(outdir / f'{save_corpus(dataset)}_wordcloud.png', dpi=200, bbox_inches='tight') # change filename as wished\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Most common words per dataframe row in bar charts and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_bar = outdir/ f'{save_corpus(dataset)}_barchart_per_csv_row/'\n",
    "outdir_bar.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "with open(outdir / f'{save_corpus(dataset)}_mostcommon_year.txt', 'a') as f:\n",
    "    print('Most common words per year in %s\\n' % (dataset), file=f)\n",
    "    for date, row in zip(data.index, data['text']):\n",
    "        # Tokenize the text into words\n",
    "        tokens = row.split()  # Alternatively, use word_tokenize(row) for more accurate tokenization\n",
    "        cleaned_tokens = remove_user_defined_stopword_list(tokens)  # Apply stopword removal\n",
    "        word_counts = Counter(cleaned_tokens)  # Count word frequencies\n",
    "        most_common_words = word_counts.most_common(number_top_words)\n",
    "\n",
    "        y = [count for word, count in most_common_words]\n",
    "        x = [word for word, count in most_common_words]\n",
    "    \n",
    "        plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "        plt.bar(x, y, color='crimson')\n",
    "        plt.title(\"Top term frequencies in \" + str(date))\n",
    "        plt.ylabel(\"Counts\")\n",
    "        #plt.yscale('log') # optionally set a log scale for the y-axis\n",
    "        plt.xticks(rotation=45)\n",
    "        for i, (word, count) in enumerate(most_common_words):\n",
    "            plt.text(i, count, f' {count} ', rotation=45,\n",
    "            ha='center', va='top' if i < 10 else 'bottom', color='white' if i < 10 else 'black')\n",
    "        plt.xlim(-0.6, len(x)-0.4) # optionally set tighter x lims\n",
    "        plt.tight_layout() # change the whitespace such that all labels fit nicely\n",
    "        plt.savefig(outdir_bar / f'{save_corpus(dataset)}_most_common_{date}.png', dpi=200, bbox_inches='tight') # change filename as wished\n",
    "        plt.show()\n",
    "        \n",
    "        print('Most common words in %s:' % (date))\n",
    "        print('Most common words in %s:' % (date), file=f)\n",
    "        for word, count in most_common_words:\n",
    "            print('%s: %7d' % (word, count))\n",
    "            print('%s: %7d' % (word, count), file=f)\n",
    "        print('\\n')\n",
    "        print('\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Visualize Total Word Frequency by Length\n",
    "\n",
    "Next, weâ€™ll analyze word frequency by word length to understand the distribution of different word lengths in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words by length\n",
    "word_lengths = {'3 letters': 0, \n",
    "                '4 letters': 0, \n",
    "                '5 letters': 0, \n",
    "                '6 letters': 0, \n",
    "                '7 letters': 0, \n",
    "                '8 letters': 0, \n",
    "                '9 letters': 0, \n",
    "                '10+ letters': 0}\n",
    "\n",
    "for word in input_as_list:\n",
    "    length = len(word)\n",
    "    if length >= 3 and length <= 9:\n",
    "        word_lengths[f\"{length} letters\"] += 1\n",
    "    elif length >= 10:\n",
    "        word_lengths[\"10+ letters\"] += 1\n",
    "\n",
    "# Plot word frequency by length\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(word_lengths.keys(), word_lengths.values())\n",
    "plt.title(\"Word Frequency by Length\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(outdir / f'{save_corpus(dataset)}_word_freq_length.png', dpi=200, bbox_inches='tight') # change filename as wished\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Print Most Common Words by Word Length\n",
    "\n",
    "For each word length category, you can examine the most common words. This can highlight patterns in the use of specific word lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length = 5 # Define word length - can be any number\n",
    "top_n = 15 # Number of top-frequency words - can be any number\n",
    "\n",
    "# Define a function to get most common words by length\n",
    "def most_common_words_by_length(tokens, word_length, top_n=top_n):\n",
    "    words = [word for word in tokens if len(word) == word_length]\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts.most_common(top_n)\n",
    "\n",
    "most_common_n_letter_words = most_common_words_by_length(input_as_list, word_length)\n",
    "\n",
    "print(\"Most common %s-letter words:\\n\" %(str(word_length)))\n",
    "for word, frequency in most_common_n_letter_words:\n",
    "    print('\\'%s\\': %s' %(word, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
