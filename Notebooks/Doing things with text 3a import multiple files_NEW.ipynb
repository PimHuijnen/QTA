{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da66f75",
   "metadata": {},
   "source": [
    "# Doing Things with Text 3a: Import and clean multiple text files\n",
    "\n",
    "This notebook introduces the automatic cleaning and saving of multiple text files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e6a00",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a library for working with text. To use it, you'll need to download some additional language data the first time you use NLTK. Run the following cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acb2e1-ade3-4647-9b3f-400e6a548835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required packages\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e0249-2479-4798-9252-a8eedc415d2d",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Packages\n",
    "\n",
    "Here, we're loading a few packages to help with text cleaning:\n",
    "- `BeautifulSoup`: To clean up HTML text.\n",
    "- `os`: Helps with interacting with the operating system, such as managing file paths and directories.\n",
    "- `re`: For regular expressions (patterns used for finding and cleaning text).\n",
    "- `nltk.tokenize`: For splitting text into individual words.\n",
    "- `nltk.corpus.stopwords`: A collection of common words like 'the', 'and', 'is', which are often removed in analysis.\n",
    "- `matplotlib.pyplot`: Allows for creating visualizations like charts and graphs to represent data visually.\n",
    "-  `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b0731",
   "metadata": {},
   "source": [
    "### Step 3: Define Input and Output Paths\n",
    "\n",
    "Define where your text file is located (input) and where you want to save your processed text (output). You will use `os.path.join()` to define your paths. This approach is cross-platform, meaning it will work on Windows, macOS, and Linux.\n",
    "\n",
    "Replace 'path', 'to', 'your', 'input', 'folder' with the actual paths to your files. It is not necessary for the output folder to exist. If it doesn't, this code will create it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = '/Path/to/indir/'\n",
    "outdir = '/Path/to/outdir'\n",
    "os.makedirs(outdir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e3a653-babf-40e4-b2d2-dbe5da7b91d1",
   "metadata": {},
   "source": [
    "### Step 3: Load text documents, clean, and write to outdir\n",
    "\n",
    "Everything that we did in notebook 1 step by step is comprimised in the next code block. First, it creates some variables that we need, next it starts a `for` loop that loops through the directory we named 'indir', opens files one by one, cleans them, and saves them to the directory named 'outdir'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c950093-f50d-427c-82e0-360c60897583",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "token_counts_before = []\n",
    "token_counts_after = []\n",
    "all_cleaned_text = []\n",
    "cleaned_data = []\n",
    "\n",
    "for filename in sorted(os.listdir(indir)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(indir, filename)\n",
    "        file_names.append(filename)\n",
    "        with open(file_path, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Cleaning steps\n",
    "        text = text.lower() # Convert text to lowercase\n",
    "        text = BeautifulSoup(text, 'html.parser').get_text() # Remove HTML tags\n",
    "        text = re.sub('[^a-z\\\\s\\']', '', text) # Remove non-alphabetic characters\n",
    "        tokens = word_tokenize(text) # Tokenize the text\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        clean_tokens = [word for word in tokens if word not in stop_words] # Remove stopwords\n",
    "        clean_tokens = [word for word in clean_tokens if len(word) >= 4] # Remove short words\n",
    "\n",
    "        token_counts_before.append(len(tokens)) # Add length of tokenized text before stop word and short word removal to token_counts_before\n",
    "        token_counts_after.append(len(clean_tokens)) # Add length of tokenized text after stop word and short word removal to token_counts_after\n",
    "        \n",
    "        cleaned_text = ' '.join(clean_tokens)\n",
    "        all_cleaned_text.append(cleaned_text)\n",
    "        cleaned_data.append({'filename': filename, 'cleaned_text': cleaned_text})\n",
    "\n",
    "        output_file_path = os.path.join(outdir, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf8') as f:\n",
    "            f.write(cleaned_text)\n",
    "        \n",
    "        print(f'Processed and saved: {filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07539374",
   "metadata": {},
   "source": [
    "### (Optional) Step 4: Save All Cleaned Text to a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73682af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset_out = dataset.replace(\" \", \"_\").lower()\n",
    "    return dataset_out\n",
    "\n",
    "merged_output_file = os.path.join(outdir, 'cleaned_text_%s.txt' %(save_dataset(dataset)))\n",
    "with open(merged_output_file, 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(all_cleaned_text))\n",
    "print('Merged cleaned text saved to:', merged_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653358c",
   "metadata": {},
   "source": [
    "### (Optional) Step 5: Save Cleaned Text in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26923e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset_out = dataset.replace(\" \", \"_\").lower()\n",
    "    return dataset_out\n",
    "\n",
    "df_cleaned = pd.DataFrame(cleaned_data)\n",
    "csv_output_file = os.path.join(outdir, 'cleaned_text_%s.csv' %(save_dataset(dataset))) \n",
    "df_cleaned.to_csv(csv_output_file, index=False, encoding='utf8')\n",
    "print('All cleaned text saved to CSV:', csv_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9f174",
   "metadata": {},
   "source": [
    "### Step 6: Count total number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820a814-6545-412c-ad93-c0e1454dd8f6",
   "metadata": {},
   "source": [
    "#### Step 6a: Visualize total number of words before and after preprocessing in a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(file_names))\n",
    "\n",
    "plt.bar(index, token_counts_before, bar_width, label='Before Cleaning')\n",
    "plt.bar([i + bar_width for i in index], token_counts_after, bar_width, label='After Cleaning')\n",
    "\n",
    "plt.xlabel('Files')\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.title(f'Token Counts Before and After Cleaning in {dataset}')\n",
    "plt.xticks([i + bar_width / 2 for i in index], file_names, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8ba19-60c3-4883-9c10-3a0c9a373bd2",
   "metadata": {},
   "source": [
    "#### Step 6b: Count total number of words in your dataset before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e67545-e578-4ce7-8c0a-164922a2e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of words in \\'%s\\' before preprocessing is: %s\" \n",
    "      %(str(indir), sum(token_counts_before)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84e7fe-d767-4e37-a1c5-d019b63198e7",
   "metadata": {},
   "source": [
    "#### Step 6c: Count total number of words in your dataset after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5152d5-ddf2-4045-b760-aae595b16a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of words in \\'%s\\' after preprocessing is: %s\" \n",
    "      %(str(indir), sum(token_counts_after)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a22c1a-a2a2-4acd-b4d6-d0f72814db2b",
   "metadata": {},
   "source": [
    "#### Step 6d: Calculate total number of words removed by preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec2b55-484b-4fce-807c-5a75723b8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of tokens removed by preprocessing is: %s\" \n",
    "      %(total_words_before - total_words_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
