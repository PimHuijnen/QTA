{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Things with Text 3b: Import and clean one or more csv files\n",
    "\n",
    "This notebook introduces the automatic cleaning and saving of one or more csv files that contains text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up NLTK\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a library for working with text. To use it, you'll need to download some additional language data the first time you use NLTK. Run the following cell once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download required packages\n",
    "import nltk\n",
    "nltk.download('punkt')  # Tokenizer\n",
    "nltk.download('stopwords')  # Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Importing Required Packages\n",
    "\n",
    "Here, we're loading a few packages to help with text cleaning:\n",
    "- `os`: Helps with interacting with the operating system, such as managing file paths and directories.\n",
    "- `csv`\n",
    "- `glob`\n",
    "- `re`: For regular expressions (patterns used for finding and cleaning text).\n",
    "- `tqdm.notebook`\n",
    "- `nltk.tokenize`: For splitting text into individual words.\n",
    "- `nltk.corpus.stopwords`: A collection of common words like 'the', 'and', 'is', which are often removed in analysis.\n",
    "-  `pandas`: Provides tools for handling and analyzing structured data in tables, making it easier to work with datasets.\n",
    "-  `matplotlib.pyplot`: Allows for creating visualizations like charts and graphs to represent data visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize  # needs to be installed first via nltk.download()\n",
    "from nltk.corpus import stopwords  # needs to be installed first via nltk.download()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Input and Output Paths\n",
    "\n",
    "Define where your text file is located (input) and where you want to save your processed text (output). You will use `os.path.join()` to define your paths. This approach is cross-platform, meaning it will work on Windows, macOS, and Linux.\n",
    "\n",
    "Replace 'path', 'to', 'your', 'input', 'folder' with the actual paths to your files. It is not necessary for the output folder to exist. If it doesn't, this code will create it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = '/Users/huijn001/desktop/test/'\n",
    "outdir = '/Users/huijn001/desktop/test1/'\n",
    "os.makedirs(os.path.dirname(outdir), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "allfiles = glob.glob(os.path.join(indir, \"*.csv\"))\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check what's in 'allfiles':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in allfiles:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 check what the data structure of csv's looks like (change 'file.csv' for one of the actual files in indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(indir + '1977.csv', sep='\\t') # most common separators are ';' or ',' or '\\t'\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['year', 'text']\n",
    "index_col = 'year' # preferably, the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Import csv's as df (with df['text']) as the text column), merge into one large dataframe called 'data'\n",
    "\n",
    "'Data' will have at least four columns:\n",
    "* the raw text from the input csv (usually ['text'])\n",
    "* the cleaned text as a list (['text_clean'])\n",
    "* the cleaned text as a string (['text_clean_str'])\n",
    "* the word count based on text_clean (['word_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: importing csv's with raw text, preprocessing including tokenization (takes time)\n",
    "\n",
    "Default is one or more csv files with the filename ending in a year (for example text_1978.csv, 1851.csv, etc.). Make sure that the separator (sep=) is correctly defined (same as above) and that the text column has the correct header (from cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Use a list to collect DataFrames\n",
    "\n",
    "for file in tqdm(allfiles):\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file, sep=\"\\t\", usecols=cols_to_keep, index_col=index_col)\n",
    "    df['text'] = df['text'].str.lower()  # Lowercase text\n",
    "\n",
    "    # Original word count per row\n",
    "    df['original_word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    # Clean text\n",
    "    df['text_clean'] = [\n",
    "        [w for w in word_tokenize(text) if w.isalpha() and len(w) >= 4] \n",
    "        for text in df['text']\n",
    "    ]\n",
    "    \n",
    "    # Cleaned word count per row\n",
    "    df['cleaned_word_count'] = df['text_clean'].str.len()\n",
    "    \n",
    "    # Words removed per row\n",
    "    df['words_removed'] = df['original_word_count'] - df['cleaned_word_count']\n",
    "    \n",
    "    # Convert cleaned text back to a string for further processing if needed\n",
    "    df['text_clean_str'] = df['text_clean'].apply(' '.join)\n",
    "\n",
    "    # Append the processed DataFrame\n",
    "    data.append(df)\n",
    "\n",
    "# Concatenate all DataFrames at once\n",
    "data = pd.concat(data, axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: importing csv's with (relatively) clean text, preprocessing without tokenization with NLTK (= quicker!)\n",
    "\n",
    "Default is one or more csv files with the filename ending in a year (for example text_1978.csv, 1851.csv, etc.). Make sure that the separator (sep=) is correctly defined (same as above) and that the text column has the correct header (from cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []  # Use a list to collect DataFrames\n",
    "\n",
    "for file in tqdm(allfiles):\n",
    "    # Load the file\n",
    "    df = pd.read_csv(file, sep=\"\\t\", usecols=cols_to_keep, index_col=index_col)\n",
    "    df['text'] = df['text'].str.lower()  # Lowercase text\n",
    "\n",
    "    # Original word count per row\n",
    "    df['original_word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    # Clean text\n",
    "    df['text'] = df['text'].str.split()\n",
    "    df['text_clean'] = [[w for w in text if w.isalpha() \n",
    "                         and len(w) >= 4] for text in df['text']]\n",
    "    \n",
    "    # Cleaned word count per row\n",
    "    df['cleaned_word_count'] = df['text_clean'].str.len()\n",
    "    \n",
    "    # Words removed per row\n",
    "    df['words_removed'] = df['original_word_count'] - df['cleaned_word_count']\n",
    "    \n",
    "    # Convert cleaned text back to a string for further processing if needed\n",
    "    df['text_clean_str'] = df['text_clean'].apply(' '.join)\n",
    "\n",
    "    # Append the processed DataFrame\n",
    "    data.append(df)\n",
    "\n",
    "# Concatenate all DataFrames at once\n",
    "data = pd.concat(data, axis=0, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save data to outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset):\n",
    "    dataset_out = dataset.replace(\" \", \"_\").lower()\n",
    "    return dataset_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: remove columns that you do not need (to make the file saved below smaller)\n",
    "\n",
    "data = data.drop(columns=[]) # add one or more columns like ['column_name_1', 'column_name_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(outdir + '%s_clean.csv' %(save_dataset(dataset)), sep ='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 6: Save dataframe with time index to outdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are only relevant if the index of 'data' above represents time. The exact formulation of 'format' below is contingent on the format of that time data (years, months and years, full dates, etc.). The code groups multiple index values with the same year together, so that the grouped dataframe can be saved as one big csv or, alternatively, as single csv's per year. It can even be saved as txt-files per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = pd.to_datetime(data.index, format =\"%Y-%m-%d\") # format depends on format in index (date) column. See https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes\n",
    "data = data.sort_index()input_as_string = \" \".join(data['text'].tolist())input_as_string = \" \".join(data['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1/3: Save df1 (grouped by year) as a single csv ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.groupby(data.index.year).sum(numeric_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(outdir + '%s_year.csv' %(save_dataset(dataset)), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2/3: Save as csv files per year ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rows = 0\n",
    "\n",
    "outdir_csv = outdir + '/%s_csv_per_year/' %(save_dataset(dataset))\n",
    "os.makedirs(os.path.dirname(outdir_csv), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "for year in range(1900,2000, 1): # make sure to fill in the correct range in years!\n",
    "    if len(data.loc[data.index.year == year]) > 0:\n",
    "        df2 = data.loc[data.index.year == year]\n",
    "        print(len(df2.index))\n",
    "        sum_rows += len(df2.index)\n",
    "        df2.to_csv(outdir_csv + str(year) + '.csv', sep='\\t')\n",
    "print(\"The number of rows in the original csv is \" + str(len(data.index)))\n",
    "print(\"The total number of rows of all new csvs is \" + str(sum_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3/3: Save as txt files per year ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_txt = outdir + '/%s_txt_per_year/' %(save_dataset(dataset))\n",
    "os.makedirs(os.path.dirname(outdir_txt), exist_ok=True) # makes outdir if it doesn't exist already\n",
    "\n",
    "for year in range(1900, 2000, 1): # make sure to fill in the correct range in years!\n",
    "    if len(data.loc[data.index.year == year]) > 0:\n",
    "        for index, row in data.iterrows():\n",
    "            if index.year == year:\n",
    "                with open(outdir_txt + str(year) + '.txt', 'a') as f:\n",
    "                    f.write(str(row[\"text_clean_str\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Count total number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7a: Visualize total number of words before and after preprocessing in a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bar width and indices for rows\n",
    "bar_width = 0.4\n",
    "index = range(len(data))  # Use the row indices as x-axis\n",
    "\n",
    "# Values for the bars\n",
    "token_counts_before = data['original_word_count'].tolist()\n",
    "token_counts_after = data['cleaned_word_count'].tolist()\n",
    "\n",
    "# Create the dual bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(index, token_counts_before, bar_width, label='Before Cleaning', alpha=0.7)\n",
    "plt.bar([i + bar_width for i in index], token_counts_after, bar_width, label='After Cleaning', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Row Index\")\n",
    "plt.ylabel(\"Word Count\")\n",
    "plt.title(\"Word Counts Before and After Cleaning Per Row\")\n",
    "plt.xticks([i + bar_width / 2 for i in index], data.index.tolist(), rotation=90)  # Use the row indices as labels\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7b: Count total number of words in your dataset before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of words in \\'%s\\' before preprocessing is: %s\" \n",
    "      %(str(indir), data['original_word_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7c: Count total number of words in your dataset after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of words in \\'%s\\' after preprocessing is: %s\" \n",
    "      %(str(indir), data['cleaned_word_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7d: Calculate total number of words removed by preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of tokens removed by preprocessing is: %s\" \n",
    "      %(data['words_removed'].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
