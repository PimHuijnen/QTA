{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 7: word2vec\n",
    "\n",
    "This notebook provides various functionalities for word embeddings with word2vec.\n",
    "\n",
    "The code assumes that the input is a series of txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 (only if needed): install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "* `unicodedata`: Works with Unicode characters and normalization\n",
    "* `re`: Provides regular expression tools for text pattern matching\n",
    "* `nltk.tokenize`: Splits text into sentences (`sent_tokenize`) and words (`word_tokenize`).\n",
    "* `matplotlib.pyplot`: Creates static, interactive, and animated visualizations\n",
    "* `matplotlib.cm`: Manages colormap settings for visualizations\n",
    "* `seaborn`: Simplifies complex data visualization based on Matplotlib\n",
    "* `numpy`: Performs numerical computations and array manipulations. `dot` and `norm` from `numpy.linalg` calculate dot products and vector norms\n",
    "* `warnings`: Manages and suppresses warning messages in Python\n",
    "* `time`: Tracks time intervals and performance measurements\n",
    "* `gensim.models.Word2Vec`: Builds and trains Word2Vec word embedding models\n",
    "* `logging`: Provides customizable logging for debugging and diagnostics\n",
    "* `sklearn.manifold.TSNE`: Reduces dimensions for high-dimensional data visualization\n",
    "* `scipy.cluster.hierarchy`: Performs hierarchical clustering and dendrogram visualization\n",
    "* `networkx`: Creates and analyzes complex networks and graph data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize  # needs to be installed first via nltk.download()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define input and output paths\n",
    "\n",
    "Define where your texts files are located (indir) and where you want to save your output (outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/outdir/')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.txt\"))\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out\n",
    "\n",
    "def to_string(list):\n",
    "    string = '_'.join(list)\n",
    "    return string\n",
    "\n",
    "def to_title(words):\n",
    "    if not words:\n",
    "        return ''\n",
    "    elif len(words) == 1:\n",
    "        return f\"'{words[0]}'\"\n",
    "    else:\n",
    "        formatted_list = [f\"'{word}'\" for word in words[:-1]]\n",
    "        return ', '.join(formatted_list) + f\" and '{words[-1]}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: with preprocessing (for raw data)\n",
    "\n",
    "Input is multiple .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "file_count = 0\n",
    "\n",
    "for infile in allfiles:\n",
    "    file_count + 1\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "    \n",
    "    # iterate through each sentence in the file\n",
    "    for i in sent_tokenize(text):\n",
    "        infile_list = []\n",
    "        for word in word_tokenize(i):\n",
    "            if len(word) > 3: # removing words of 3 letters and shorter\n",
    "                new_word = re.sub(r'[^\\w\\s]', '', word) # preprocessing\n",
    "                if new_word != '':\n",
    "                    infile_list.append(new_word.lower())\n",
    "        if infile_list != '':\n",
    "            data.append(infile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: without preprocessing (for preprocessed data - much quicker)\n",
    "\n",
    "Input is multiple .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "file_count = 0\n",
    "\n",
    "for count, infile in enumerate(allfiles):\n",
    "    file_count += 1\n",
    "    # open the file and do something with it, close when done\n",
    "    with open(infile, \"r\") as f:\n",
    "        # try / except clause to catch encoding errors\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "    infile_list = [x for x in text.split(' ') if len(x) > 3] # removing words of 3 letters and shorter\n",
    "    print('%s has %s words' %(infile, len(infile_list)))\n",
    "    data.append(infile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that list 'data' contains as many lists as there are files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List \\'data\\' is %s lists long, which equals the %s files in indir' %(len(data), file_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a word2vec model\n",
    "\n",
    "from: https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(data, min_count=5, vector_size=128, workers=3, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Step 4a: Save model to outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(outdir + dataset + \"_w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Search most similar terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['word'] # can be one or more words as 'word', 'word', 'word'\n",
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    most_similar = model.wv.most_similar(positive=[key], topn=n)\n",
    "    print('Words most similar to \\'%s\\':'%(key))\n",
    "    for word in most_similar:\n",
    "        print(word)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize most similar words as clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, score in model.wv.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot_similar_words('Word clusters for %s' %(to_title(keys)), \n",
    "                        keys, \n",
    "                        embeddings_en_2d, \n",
    "                        word_clusters, \n",
    "                        0.7, \n",
    "                        str(outdir) + dataset + '_%s_tsne.png' %(to_string(keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Visualize most similar words in a dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram_for_similar_words(search_term, model, top_n=100):\n",
    "    # Retrieve the most similar words\n",
    "    similar_words = model.wv.most_similar(search_term, topn=top_n)\n",
    "    words = [word for word, _ in similar_words]\n",
    "    \n",
    "    # Include the search term itself\n",
    "    words.append(search_term)\n",
    "    \n",
    "    # Get the word vectors\n",
    "    word_vectors = [model.wv[word] for word in words]\n",
    "    \n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(word_vectors, 'ward')\n",
    "    \n",
    "    # Plot the dendrogram\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    dendrogram(linkage_matrix, labels=words, leaf_rotation=90)\n",
    "    plt.title(f\"Dendrogram for the top {top_n} words similar to '{search_term}'\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"word\"\n",
    "plot_dendrogram_for_similar_words(search_term, model, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Visualize most similar words in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap_for_similar_words(search_term, model, top_n=100):\n",
    "    # Retrieve the most similar words\n",
    "    similar_words = model.wv.most_similar(search_term, topn=top_n)\n",
    "    words = [word for word, _ in similar_words]\n",
    "    \n",
    "    # Include the search term itself\n",
    "    words.append(search_term)\n",
    "    \n",
    "    # Get the word vectors\n",
    "    word_vectors = np.array([model.wv[word] for word in words])\n",
    "    \n",
    "    # Compute the cosine similarity matrix\n",
    "    similarities = np.inner(word_vectors, word_vectors)\n",
    "    \n",
    "    # Normalize the similarity values to the range [0, 1]\n",
    "    norms = np.linalg.norm(word_vectors, axis=1)\n",
    "    similarities = similarities / norms[:, np.newaxis] / norms[np.newaxis, :]\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(similarities, xticklabels=words, yticklabels=words, cmap='coolwarm', annot=False)\n",
    "    plt.title(f\"Cosine Similarity Heatmap for the top {top_n} words similar to '{search_term}'\")\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"word\"\n",
    "plot_heatmap_for_similar_words(search_term, model, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Visualize most similar words as a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(model, keyword, top_n):\n",
    "    similar_words = model.wv.most_similar(keyword, topn=top_n)\n",
    "    return [word for word, _ in similar_words]\n",
    "\n",
    "def build_network(model, seed_words, top_n):\n",
    "    network = nx.Graph()\n",
    "\n",
    "    for seed_word in seed_words:\n",
    "        similar_words = get_similar_words(model, seed_word, top_n)\n",
    "        network.add_node(seed_word)\n",
    "\n",
    "        for word in similar_words:\n",
    "            network.add_node(word)\n",
    "            network.add_edge(seed_word, word)\n",
    "\n",
    "            second_degree_words = get_similar_words(model, word, top_n2)\n",
    "            for second_word in second_degree_words:\n",
    "                network.add_node(second_word)\n",
    "                network.add_edge(word, second_word)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(network):\n",
    "    node_sizes = [d * 150 for n, d in network.degree()]\n",
    "\n",
    "    #pos = nx.shell_layout(network)\n",
    "    #pos = nx.circular_layout(network)\n",
    "    # Use Fruchterman-Reingold layout\n",
    "    pos = nx.spring_layout(network, seed=42)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    nx.draw_networkx_nodes(network, pos, node_size=node_sizes, node_color='skyblue')\n",
    "    nx.draw_networkx_edges(network, pos, width=1.0, alpha=0.5)\n",
    "    nx.draw_networkx_labels(network, pos, font_size=10, font_color='black')\n",
    "    \n",
    "    plt.title(\"Network of most similar words of %s in %s\" %(to_title(seed_words), dataset))\n",
    "    plt.savefig(str(outdir / (\"%s_%s_%s_network.png\" % (to_string(seed_words), str(top_n), str(top_n2)))), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words = ['word']\n",
    "top_n = 10\n",
    "top_n2 = 2\n",
    "network_words = build_network(model, seed_words, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_network(network_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
