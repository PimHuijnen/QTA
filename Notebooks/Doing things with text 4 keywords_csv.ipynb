{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing things with text 4: Keywords (for csvs)\n",
    "\n",
    "This notebook provides various functionalities for keyword searches in a corpus of (cleaned) texts:\n",
    "* keyword frequency\n",
    "* n-grams\n",
    "* collocations\n",
    "\n",
    "The code assumes that the input is a series of txt files with the date (preferably years) as titles, as in, for example, 1980.txt, 1981.txt, 1982.txt, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pathlib.Path`: Provides an object-oriented interface for filesystem paths\n",
    "- `pandas`: Provides data structures and tools for data manipulation and analysis.\n",
    "- `collections.defaultdict`: Creates dictionaries with default values for missing keys.\n",
    "- `collections.Counter`: Counts occurrences of elements in an iterable.\n",
    "- `datetime`: Supplies classes for manipulating dates and times.\n",
    "- `nltk.util.ngrams`: Generates n-grams from a sequence.\n",
    "- `nltk.bigrams`: Creates bigrams (2-word combinations) from a sequence.\n",
    "- `nltk.collocations`: Provides tools for identifying collocations (frequent word pairings).\n",
    "- `nltk.FreqDist`: Calculates frequency distributions of items in a dataset.\n",
    "- `nltk.collocations.*`: Includes utilities for finding collocations like bigram or trigram associations.\n",
    "- `nltk.WordPunctTokenizer`: Tokenizes text into words and punctuation marks.\n",
    "- `matplotlib.pyplot`: Creates static, animated, and interactive visualizations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk import collocations\n",
    "from nltk import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk import WordPunctTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define input and output paths\n",
    "\n",
    "Define where your text file is located (indir) and where you want to save your processed text (outdir)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output paths\n",
    "indir = Path('/Path/to/indir/')\n",
    "outdir = Path('/Path/to/indir/')\n",
    "outdir.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "allfiles = sorted(indir.glob(\"*.csv\"))\n",
    "\n",
    "dataset = 'dataset' # here the name of your actual dataset for output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in allfiles:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(indir / 'dataset_clean.csv', sep='\\t')\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus(corpus):\n",
    "    corpus_out = corpus.replace(\" \", \"_\").lower()\n",
    "    return corpus_out\n",
    "\n",
    "def to_string(list):\n",
    "    string = '_'.join(list)\n",
    "    return string\n",
    "\n",
    "def to_title(words):\n",
    "    if not words:\n",
    "        return ''\n",
    "    elif len(words) == 1:\n",
    "        return f\"'{words[0]}'\"\n",
    "    else:\n",
    "        formatted_list = [f\"'{word}'\" for word in words[:-1]]\n",
    "        return ', '.join(formatted_list) + f\" and '{words[-1]}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the data into a dataframe \n",
    "Df with the texts in \"text_clean_str\" column and the file name (=date) in \"date\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3a: Create dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(list)\n",
    "\n",
    "for file in allfiles:\n",
    "    try:\n",
    "        # Load the CSV into a DataFrame\n",
    "        df_infile = pd.read_csv(file, sep='\\t')\n",
    "        \n",
    "        # Ensure 'date' and 'text' are available\n",
    "        if 'date' in df_infile.columns:\n",
    "            date_column = df_infile['date']\n",
    "        else:\n",
    "            date_column = df_infile.index  # Use index if 'date' column doesn't exist\n",
    "\n",
    "        # Append the data to results\n",
    "        for date, text in zip(date_column, df_infile['text_clean_str']):\n",
    "            results[\"date\"].append(date)\n",
    "            results[\"text\"].append(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Convert the results dictionary into a DataFrame\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3b: Turn \"year\" column into datetime and set as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"date\")\n",
    "df.index = pd.to_datetime(df.index, format =\"%d-%m-%Y\")\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3c: Group by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'year' and aggregate 'text' by concatenation and 'num_words' by summation\n",
    "df = df.groupby(df.index.year).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Step 4: Removing custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['word', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_word_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [[x for x in text.split(' ') if len(x) >= minimum_word_length and x not in stopwords] \n",
    "              for text in df['text']]\n",
    "df['text'] = df['text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Count and plot total words per year (or document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    words = string.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words'] = df['text'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,8))\n",
    "plt.bar(df_year.index, df_year['num_words'])\n",
    "plt.ylabel('words')\n",
    "plt.xlabel('date')\n",
    "plt.xticks(df.index.year, rotation=45)\n",
    "plt.title(\"Total number of words per year in %s\" %(dataset))\n",
    "plt.savefig(outdir / f'total_words_{save_corpus(dataset)}.png', dpi=200, bbox_inches='tight') # change filename as wished)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Finding and visualizing (ngram) strings in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = ['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_freq = f'{to_string(search_words)}_{save_corpus(dataset)}_freq.png'\n",
    "outdir_freq = outdir / f'{save_corpus(dataset)}_keyword_trends/'\n",
    "outdir_freq.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_freq = outdir_freq / outfile_freq\n",
    "\n",
    "fig = plt.figure(figsize = (15,8))\n",
    "\n",
    "for search_word in search_words:\n",
    "    df[search_word + '_rel'] = df.text.str.count(pat=search_word + '??') / df.num_words\n",
    "    plt.scatter(df.index.year, df[search_word + '_rel'], label=search_word)\n",
    "    with open(outdir_freq / f'{search_word}_{save_corpus(dataset)}_freq.txt', 'a') as f:\n",
    "        print('Relative frequency of \\'%s\\' in %s\\n' %(search_word, dataset), file=f)\n",
    "        print(df.text.str.count(pat=search_word + '??') / df.num_words, file=f)\n",
    "\n",
    "plt.ylabel('frequency')\n",
    "plt.xlabel('year')\n",
    "plt.title(\"Keyword trends in %s\" %(dataset))\n",
    "plt.legend()\n",
    "plt.xticks(df.index.year, rotation=45)\n",
    "plt.savefig(outpath_freq)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_freq = f'{to_string(search_words)}_{save_corpus(dataset)}_freq.png'\n",
    "outdir_freq = outdir / f'{save_corpus(dataset)}_keyword_trends/'\n",
    "outdir_freq.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_freq = outdir_freq / outfile_freq\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "for search_word in search_words:\n",
    "    # Calculate relative frequency\n",
    "    df[search_word + '_rel'] = df.text.str.count(pat=search_word + '??') / df.num_words\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.scatter(df.index.year, df[search_word + '_rel'], label=search_word)\n",
    "    \n",
    "    # Fit a trend line (linear regression)\n",
    "    x = df.index.year\n",
    "    y = df[search_word + '_rel']\n",
    "    mask = ~np.isnan(y)  # Exclude NaN values if any\n",
    "    z = np.polyfit(x[mask], y[mask], 1)  # Fit a line (1st degree polynomial)\n",
    "    p = np.poly1d(z)  # Create a polynomial function\n",
    "    \n",
    "    # Plot the trend line\n",
    "    plt.plot(x, p(x), linestyle='--', label=f\"{search_word} trend\")\n",
    "    \n",
    "    # Save relative frequencies to file\n",
    "    with open(outdir_freq / f'{search_word}_{save_corpus(dataset)}_freq.txt', 'a') as f:\n",
    "        print('Relative frequency of \\'%s\\' in %s\\n' % (search_word, dataset), file=f)\n",
    "        print(df.text.str.count(pat=search_word + '??') / df.num_words, file=f)\n",
    "\n",
    "# Final plot settings\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Year')\n",
    "plt.title(f\"Keyword trends in {dataset}\")\n",
    "plt.legend()\n",
    "plt.xticks(df.index.year, rotation=45)\n",
    "plt.savefig(outpath_freq)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7a: Finding and printing word endings in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ending = 'heid'\n",
    "min_freq_end = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_end = f'top_endings_{ending}_{save_corpus(dataset)}.txt'\n",
    "outdir_end = outdir / f'{save_corpus(dataset)}_endings_beginnings'\n",
    "outdir_end.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_end = outdir_end / outfile_end\n",
    "\n",
    "\n",
    "with open(outpath_end, 'a') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        print(str(index)[:4])\n",
    "        print(str(index)[:4], file=f)\n",
    "        word_counts = Counter(row['text'].split())\n",
    "        word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "        for word, count in word_counts.items():\n",
    "            if word.endswith(ending) and count >= min_freq_end:\n",
    "                print(word + \": \", count)\n",
    "                print(word + \": \", count, file=f)\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7b: Finding and printing word beginnings in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = 'pre'\n",
    "min_freq_begin = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_begin = f'top_beginnings_{begin}_{save_corpus(dataset)}.txt'\n",
    "outdir_begin = outdir / f'{save_corpus(dataset)}_endings_beginnings'\n",
    "outdir_begin.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_begin = outdir_begin / outfile_begin\n",
    "\n",
    "with open(outpath_begin, 'a') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        print(str(index)[:4])\n",
    "        print(str(index)[:4], file=f)\n",
    "        word_counts = Counter(row['text'].split())\n",
    "        word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}\n",
    "        for word, count in word_counts.items():\n",
    "            if word.startswith(begin) and count >= min_freq_begin:\n",
    "                print(word, \": \", count)\n",
    "                print(word, \": \", count, file=f)\n",
    "        print(\"\\n\")\n",
    "        print(\"\\n\", file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Collocations\n",
    "\n",
    "From: https://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['word']\n",
    "windows = [10] # add or change to smaller/larger window\n",
    "algorithms = ['likelihood', 'pmi'] # 'likelihood', 'pmi', 'raw_freq'\n",
    "coll_to_print = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir_coll = outdir / f'{save_corpus(dataset)}_collocations'\n",
    "outdir_coll.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "for term in search_terms:\n",
    "    for window in windows:\n",
    "        for algorithm in algorithms:\n",
    "            outfile_coll = f'{term}_{algorithm}_collocations_{window}.txt'\n",
    "            outpath_coll = outdir_coll / outfile_coll\n",
    "            \n",
    "            with open(outpath_coll, 'a') as f:\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset))\n",
    "                print('Top %s %s collocations of \\'%s\\' with a window of %s in %s:\\n' %(str(coll_to_print), algorithm, term, str(window), dataset), file=f)\n",
    "                for year, doc in zip(df.index.year, df.text):\n",
    "                    tokens = WordPunctTokenizer().tokenize(doc)\n",
    "            \n",
    "                    bigram_measures = collocations.BigramAssocMeasures()\n",
    "                    word_fd = FreqDist(tokens)\n",
    "                    bigram_fd = FreqDist(bigrams(tokens))\n",
    "                    finder = BigramCollocationFinder(word_fd, bigram_fd, window_size=window)\n",
    "\n",
    "                    #preprocessing: remove short words and stop words (see above) if only relevant for collocations\n",
    "                    #finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in stopwords)\n",
    "        \n",
    "                    if algorithm == 'likelihood': \n",
    "                        scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "                    elif algorithm == 'pmi': \n",
    "                        scored = finder.score_ngrams(bigram_measures.pmi) \n",
    "                    else: \n",
    "                        scored = finder.score_ngrams(bigram_measures.raw_freq) \n",
    "\n",
    "                    # Group bigrams by first word in bigram                                       \n",
    "                    prefix_keys = defaultdict(list)\n",
    "                    for key, scores in scored:\n",
    "                        prefix_keys[key[0]].append((key[1], scores))\n",
    "\n",
    "                    # Sort keyed bigrams by strongest association                                  \n",
    "                    for key in prefix_keys:\n",
    "                        prefix_keys[key].sort(key = lambda x: -x[1])\n",
    "\n",
    "                    # Print top collocations of term.\n",
    "                    print(str(year) + ':')\n",
    "                    print(str(year) + ':', file=f)\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n')\n",
    "                    print(*prefix_keys[term][:coll_to_print], sep='\\n', file=f)\n",
    "                    print('\\n')\n",
    "                    print('\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: N-grams - most common; with given start word; with given end word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define length of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 2\n",
    "words_to_print = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9a: Print and save to disk the most common ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_ngram = f'{save_corpus(dataset)}_mostcommon_{ngram}_grams.txt'\n",
    "outdir_ngram = outdir / f'{save_corpus(dataset)}_ngrams'\n",
    "outdir_ngram.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_ngram = outdir_ngram / outfile_ngram\n",
    "\n",
    "with open(outpath_ngram, 'a') as f:\n",
    "\n",
    "    print('Top ' + str(words_to_print) + ' ' + str(ngram) + '-grams in ' + dataset + ':\\n')\n",
    "    print('Top ' + str(words_to_print) + ' ' + str(ngram) + '-grams in ' + dataset + ':\\n', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print) # for n see above\n",
    "        \n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        print(*top_grams, sep='\\n')\n",
    "        print(*top_grams, sep='\\n', file=f)\n",
    "        print('\\n')\n",
    "        print('\\n', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9b: Print and save to disk the most common ngrams beginning with a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginword = 'albert' # type as 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_ngramend = f'{save_corpus(dataset)}_{beginword}_{ngram}_grams_begin.txt'\n",
    "outdir_ngram = outdir / f'{save_corpus(dataset)}_ngrams'\n",
    "outdir_ngram.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_ngramend = outdir_ngram / outfile_ngramend\n",
    "\n",
    "with open(outpath_ngramend, 'a') as f:\n",
    "    print(f'Top', str(ngram) + '-grams beginning with \\'' + beginword + '\\' in ' + dataset + ':\\n')\n",
    "    print(f'Top', str(ngram) + '-grams beginning with \\'' + beginword + '\\' in ' + dataset + ':\\n', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print)\n",
    "        for item in top_grams:\n",
    "            if item[0][0] == beginword:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=f)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9c: Print and save to disk the most common ngrams ending with a particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endword = 'heijn' # type as 'word'\n",
    "end = ngram - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_ngrambegin = f'{save_corpus(dataset)}_{endword}_{ngram}_grams_end.txt'\n",
    "outdir_ngram = outdir / f'{save_corpus(dataset)}_ngrams'\n",
    "outdir_ngram.mkdir(parents=True, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "outpath_ngrambegin = outdir_ngram / outfile_ngrambegin\n",
    "\n",
    "with open(outpath_ngrambegin, 'a') as f:\n",
    "    print(f'Top', str(ngram) + '-grams ending with \\'' + endword + '\\' in ' + dataset + ':')\n",
    "    print(f'Top', str(ngram) + '-grams ending with \\'' + endword + '\\' in ' + dataset + ':', file=f)\n",
    "\n",
    "    for year, text in zip(df.index.year, df.text):\n",
    "        print(year)\n",
    "        print(year, file=f)\n",
    "        grams = ngrams(text.split(), ngram)\n",
    "        grams_freq = Counter(grams)\n",
    "        top_grams = grams_freq.most_common(words_to_print)\n",
    "        for item in top_grams:\n",
    "            if item[0][end] == endword:\n",
    "                print(item, sep='\\n')\n",
    "                print(item, sep='\\n', file=f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
